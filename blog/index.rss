<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[strugee.net blog]]></title><description><![CDATA[strugee.net blog]]></description><link>https://strugee.net/blog/</link><generator>stratic-indexes-to-rss</generator><lastBuildDate>Thu, 06 Aug 2020 01:40:25 GMT</lastBuildDate><atom:link href="https://strugee.net/blog//index.rss" rel="self" type="application/rss+xml"/><copyright><![CDATA[Â© Copyright 2012-2018 AJ Jordan. Available under the GNU Affero GPL.]]></copyright><webMaster><![CDATA[AJ Jordan <alex@strugee.net>]]></webMaster><item><title><![CDATA[filter-other-days 1.1.0 and 2.0.0 are now available]]></title><description><![CDATA[<p>I'm pleased to announce that <code>filter-other-days</code> 1.1.0 and 2.0.0 are now available. In fact, they were both released simultaneously over three weeks ago while I was at <a href="https://seagl.org/">SeaGL</a>, but things at college have been so hectic I'm only just finding time to write about them! If you're not already familiar with <code>filter-other-days</code> - which provides reliable, Artificial Ignorance-compatible logfile date filtering even in the face of unknown formats - I would encourage you to read my blog post <a href="https://strugee.net/blog/2017/10/announcing-filter-other-days">introducing the tool for the first time</a>. Or, if you read this post late enough, you could even watch video of the <a href="https://osem.seagl.org/conferences/seagl2019/program/proposals/682">talk I gave at SeaGL</a>, which talks about <code>filter-other-days</code> before pivoting into a broader discussion of the kind of runaway complexity <code>filter-other-days</code> is designed to address.</p>
<p>1.1.0 and 2.0.0 are both feature releases. Due to <code>filter-other-days</code> 2.0.0 breaking compatibility with OpenBSD, I'm providing 1.1.0 which contains everything that <code>filter-other-days</code> 2.0.0 does <em>except</em> for the feature that breaks OpenBSD support - localization in the logfile filters. (Localization does not and to my knowledge cannot work on OpenBSD because OpenBSD does not support the POSIX features that <code>filter-other-days</code>' localization relies on.)</p>
<p>Here are the highlights of the engineering that both 1.1.0 and 2.0.0 share:</p>
<ul>
<li><code>filter-other-days -d</code> operates on any day instead of the current date on supported systems</li>
<li><code>filter-other-days</code> is portable to OpenBSD, NetBSD, OpenIndiana and OmniOS (i.e. illumos), and Cygwin</li>
<li>GNU <code>seq</code> is no longer required; the only requirement for core functionality is now POSIX</li>
<li>Several bugs have been fixed</li>
<li>Release artifacts are built <a href="https://reproducible-builds.org/">reproducibly</a></li>
<li>Automated testing has been improved</li>
</ul>
<p>Note that <code>filter-other-days -d</code> <em>does</em> require more than POSIX - it needs a system with either GNU <code>date -d</code> semantics or BSD <code>date -r</code> semantics. This is because POSIX does not provide enough support to implement this feature otherwise. If your system does not support either of these, <code>filter-other-days</code> will simply turn the feature off. You can check if <code>-d</code> is available by looking for it in the help output - it will show up only if the system supports it.</p>
<p>In addition to the above, <code>filter-other-days</code> 2.0.0 <em>also</em> includes support for filtering logfiles in different locales. This means that if your system logs things like month names in languages other than English, <code>filter-other-days</code> will now be able to process these logs! <code>filter-other-days</code> will automatically use the C locale (which is mandated to be available by POSIX) and will additionally use the locale defined by the <code>$LANG</code> environment variable, if set. You can also specify more locales to be loaded by specifiying the <code>-l</code> command line flag. <code>filter-other-days</code> extracts the information it needs using specific keywords in the system locales, which means that if you want <code>filter-other-days</code> to load a particular locale to filter with, you need to have that locale installed.</p>
<p>Unfortunately, there's one more complication: some systems are buggy and do not have keywords that properly conform to POSIX. <a href="https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=237752">FreeBSD 12.0 and below</a> as well as <a href="https://gnats.netbsd.org/cgi-bin/query-pr-single.pl?number=54693">NetBSD</a> are known to have these bugs. Since these systems are relatively popular, I am providing patched versions of <code>filter-other-days</code> that will work around these bugs. You can recognize these versions because they have <code>freebsd</code> in the tarball filename. They will also tell you they've been patched for FreeBSD (and NetBSD) in all relevant places, like the version output and the manpage.</p>
<p>So, to summarize what version to use:</p>
<ul>
<li>If you're on OpenBSD, use 1.1.0</li>
<li>If you're on FreeBSD 12.0 or below, or on NetBSD, use 2.0.0 with FreeBSD patches</li>
<li>Otherwise, use the unpatched 2.0.0 tarballs</li>
</ul>
<p>I hope these releases of <code>filter-other-days</code> are useful to people! I'm super proud of them and I couldn't be more excited for people to try them out. And as always, feel free to <a href="https://github.com/strugee/filter-other-days/issues">report any bugs</a> you find!</p>
]]></description><link>https://strugee.net/blog/2019/12/filter-other-days-1.1.0-and-2.0.0-are-now-available</link><guid isPermaLink="true">https://strugee.net/blog/2019/12/filter-other-days-1.1.0-and-2.0.0-are-now-available</guid><category><![CDATA[development]]></category><category><![CDATA[releases]]></category><category><![CDATA[sysadmin]]></category><category><![CDATA[blaggregator]]></category><pubDate>Wed, 11 Dec 2019 17:45:24 GMT</pubDate></item><item><title><![CDATA[pump.io DOMPurify security fixes available]]></title><description><![CDATA[<p>Recently the cross-site-scripting sanitization library that pump.io uses, <a href="https://github.com/cure53/DOMPurify">DOMPurify</a>, published several security advisories for mXSS vulnerabilities affecting browsers based on the Blink rendering engine - you can find the latest one, for example, <a href="https://lists.ruhr-uni-bochum.de/pipermail/dompurify-security/2019-October/000012.html">here</a>. As we've done in the past, the pump.io project is publishing security releases to ensure that everyone is using the latest version of DOMPurify. Per our <a href="https://github.com/pump-io/pump.io/wiki/Security">security support policy</a>, we are providing patches for the current stable release and the previous stable release:</p>
<ol>
<li>pump.io 5.1.2 has been updated to pump.io 5.1.3</li>
<li>pump.io 5.0.2 has been updated to pump.io 5.0.3</li>
</ol>
<p>As these are security releases we encourage administrators to upgrade as soon as possible. Both 5.1.3 and 5.0.3 are drop-in replacements for their predecessors. If you have pump.io 5.1 installed via npm - our recommended configuration - you can upgrade with:</p>
<pre><code>$ npm install -g pump.io@5
</code></pre>
<p>If you're on pump.io 5.0, we recommend that you also run the above command to upgrade to 5.1 - it's a drop-in replacement for 5.0. However, if you want to stick with 5.0 for the time being, you can install a patched version with:</p>
<pre><code>$ npm install -g pump.io@5.0
</code></pre>
<p>Note that if you have a source-based install, the above commands won't work and you will need to upgrade however you usually do - this will depend on how exactly you have pump.io set up.</p>
<p>If you need help, or if you have questions about these security releases, get in touch with <a href="https://github.com/pump-io/pump.io/wiki/Community">the community</a>.</p>
]]></description><link>https://strugee.net/blog/2019/10/pump.io-dompurify-security-fixes-available</link><guid isPermaLink="true">https://strugee.net/blog/2019/10/pump.io-dompurify-security-fixes-available</guid><category><![CDATA[pump.io]]></category><category><![CDATA[development]]></category><category><![CDATA[security]]></category><category><![CDATA[releases]]></category><category><![CDATA[blaggregator]]></category><pubDate>Tue, 15 Oct 2019 18:28:01 GMT</pubDate></item><item><title><![CDATA[Make ReCaptcha's "I'm not a robot" accurate]]></title><description><![CDATA[<p>A month or two ago, my friend Evan <a href="https://twitter.com/evanpro/status/1098367574969077761">tweeted</a>:</p>
<blockquote>
<p>Fuck reCaptcha.</p>
<p>I am sick of doing unpaid labour classifying images for Google.</p>
<p>We need a captcha widget that contributes to the global commons instead of siphoning value into yet another proprietary lockbox.</p>
</blockquote>
<p>Frankly I agree. Not only am I being forced to do Google's dirty work, but ReCaptcha is known to make life extremely difficult for Tor users. I've literally spent 15 minutes before trying to solve a stupid captcha and eventually I gave up because Google just wouldn't let me past. ReCaptcha profits off of innocent users who are just trying to go about their business on the web, and Google is exploiting people in order to build a locked-up, proprietary image recognition system. Why are we, the users, not allowed to have access to the fruits of the labor that we are forced to provide for free? Because of this, today I am announcing an advanced, next-generation, cutting-edge platform that is poised to <em>revolutionize</em> this problem space.</p>
<p>Nah, just kidding (mostly). Inspired by <a href="https://twitter.com/evanpro/status/1098987608020008961">another tweet from Evan</a>, I threw together a browser extension in like 30 minutes that changes "I'm not a robot" to "I want to do unpaid image classification". After a long exchange with the fine folks behind addons.mozilla.org (because my account is so old that when I logged in I hit <a href="https://github.com/mozilla/addons-server/issues/8765">this bug</a> and got 500 Internal Server Error), I've finally sorted out my logins, and today I've uploaded the latest version of this extension to both addons.mozilla.org and the Chrome Web Store. So at least if you're getting screwed by Google, you'll be able to make them be honest about how they're screwing you. Here's a screenshot:</p>
<p><img src="/images/recaptcha-screenshot.jpg" alt="Screenshot of a test page with Google ReCaptcha on it; &quot;I'm not a robot&quot; has been replaced with &quot;I would like to do unpaid image classification&quot;"></p>
<p>Also, for some extra irony, I took the icon from Google's Apache 2.0-licensed <a href="https://material.io/tools/icons/?search=image_search&amp;icon=image_search&amp;style=baseline">Material Design icon set</a>. If someone feels like contributing a real icon, I would probably replace the current icon with it. (I would also be happy to take translations into different languages!)</p>
<p>In the words of the README:</p>
<blockquote>
<p>This was Evan Prodromou's idea unless you like it, in which case the idea to follow through and make an actual extension was totally all mine.</p>
</blockquote>
<p>I hope people enjoy this extension! <em>Make ReCaptcha's "I'm not a robot" accurate</em> is available as a <a href="https://addons.mozilla.org/en-US/firefox/addon/make-recaptcha-text-accurate/">Firefox extension</a> and as a <a href="https://chrome.google.com/webstore/detail/make-recaptchas-im-not-a/olbjmgkbokabjfaelgogjjllpnfjgdoe">Chrome extension</a>, and the source is published <a href="https://github.com/strugee/recaptcha-unpaid-labor">on GitHub</a>.</p>
]]></description><link>https://strugee.net/blog/2019/04/make-recaptchas-im-not-a-robot-accurate</link><guid isPermaLink="true">https://strugee.net/blog/2019/04/make-recaptchas-im-not-a-robot-accurate</guid><category><![CDATA[development]]></category><category><![CDATA[blaggregator]]></category><pubDate>Sat, 20 Apr 2019 22:05:58 GMT</pubDate></item><item><title><![CDATA[Legacy]]></title><description><![CDATA[<!-- Written to a subset of the World of Goo soundtrack -->
<p>Recently I turned 20 years old. As the days continue to go by, I have been struggling with the concept of time.</p>
<p>Probably this feeling started with me seeing photos of this year's SAAS robotics teams. I was on the robotics team for four years (I've <a href="https://strugee.net/blog/2014/12/new-blog-new-site">written about it a little before</a>) but recently I found that all of a sudden, I know barely anyone in the photos. Fifteen people in robotics graduated[1] high school last year, people I've largely known since they were freshmen (or in the case of <a href="https://jessewalling.com/">Jesse</a> and <a href="https://matthewkelsey.com/">Matt</a>, even longer). Some of them are very close friends. And I have found that the way I think about <a href="http://www.redshiftrobotics.org/">Redshift Robotics</a> has changed: I don't quite think of it as something I used to be a part of, at least not that simply. Instead I think of it as my legacy.</p>
<blockquote>
<p>legacy: <em>n.</em> something handed down from an ancestor or a predecessor or from the past</p>
</blockquote>
<p><cite>- from The American HeritageÂ® Dictionary of the English Language, 4th Edition, via <a href="https://www.wordnik.com/words/legacy">Wordnik</a></cite></p>
<p>I don't belong there anymore. FIRST Tech Challenge has changed since I've left. They play a different game than I did, they run their GitHub differently than I envisioned when I set it up, they have different mentors and leadership than we did, and I'm sure they practice and compete differently than we did. That's okay. But it's weird. I know exactly what they're thinking because I thought it too, years and years ago - to them, I am a vague echo in someone else's past; an accumulation of events that has shaped what the club is today but that they would never be able to name. I'm someone they've vaguely heard of, maybe, but never really known. What happens next is all in their hands and there's nothing I can or want to do about it; I witnessed a former mentor of SAAS robotics turn the entire club around and that's history they'll never have seen, and maybe will never even know about. But whatever is happening there now is something <em>I'll</em> never see, either.</p>
<p>Pretty soon there will be no students left at SAAS I still know. Two more very close friends graduate this year. It'll only take a year or two more after that before the last student I know personally is gone.</p>
<blockquote>
<p>There are places I remember all my life</br>
Though some have changed<br/>
Some forever, not for better<br/>
Some have gone and some remain.<br/></p>
</blockquote>
<p><cite>- The Beatles, In My Life</cite></p>
<p>I think maybe as time goes on I'm getting better at watching people grow and change. I'm on year two of college and already I see it happening - last year a close group of friends and I were just freshmen, bright-eyed and bushy-tailed. This year one's an RA, two are First-year Fellows[2] and I'm a D'Lion[3]. The rest are living their lives in upper-class housing. As I was sitting in the dining hall earlier tonight I thought, how lucky we are that we even met last year. We're all scattered around now, generally in wildly different departments, and we're all focusing on different next steps. Most of us live in different buildings from all the others, and I can't help but wonder if we would all have met later. I think the answer is probably no. Causality is a strange thing to me; the circumstances that brought us together are dead and gone and yet the friendship lives on. Legacy.</p>
<p>I see the world changing too. There was a point in my life where it seemed like nothing big and cosmic would ever change; it seemed like the world would adjust itself only in small details. But at some point that stopped being the case, when I wasn't watching. <a href="https://www.jamesporter.me/">James Porter</a>, who's part of the faculty at the <a href="https://www.recurse.com/">Recurse Center</a>, once told me that whenever new batches came in he was always super sad to see so many people he'd made friends with leave to be replaced by strangers he didn't know yet. But, he said, he never quite noticed the point where people in-batch switched from strangers to friends. That feeling of not noticing is how I feel about the world right now. And it just boggles the mind that I'm at an age now where I can actually say yeah, I've seen those things change. I see people even just a year or two younger than me so much more attached to their phones; I say "back in the day" unironically when I talk about tech things.</p>
<p>The traffic in Seattle has gotten worse, almost unbearable compared to what it once was. We didn't used to hear about wildfires like we do now; the country and the planet are <em>burning</em>. And above all else, <a href="https://strugee.net/blog/2016/12/where-were-headed-aka-im-worried">the country is angrier</a>. So much angrier.</p>
<blockquote>
<p>I turned 20 today. A question for people older and wiser than I: I feel incredibly weird to not be a "teenager" anymore, what the hell do I do???</p>
</blockquote>
<p><cite>- me, a couple days ago, on <a href="https://pump.strugee.net/alex/note/VRge4hnIQ1WM-Z1cJxwifA">pump.io</a> and on <a href="https://twitter.com/strugee2/status/1036789205245018112">Twitter</a></cite></p>
<p>Footnotes:</p>
<p>[1]: and we all know how I feel about <a href="https://strugee.net/blog/2017/06/graduation-2017-reflections-on-365-days-of-gap-yearing">graduations</a>.</p>
<p>[2]: upper-class student at University of Rochester who lives with first-years and helps them with academic stuff (especially course registration)</p>
<p>[3]: like a First-year Fellow except instead of academic stuff, they help bring their hall together and make sure everyone's having a good time and making friends, etc.</p>
]]></description><link>https://strugee.net/blog/2018/09/legacy</link><guid isPermaLink="true">https://strugee.net/blog/2018/09/legacy</guid><category><![CDATA[personal]]></category><category><![CDATA[blaggregator]]></category><pubDate>Sun, 09 Sep 2018 23:01:49 GMT</pubDate></item><item><title><![CDATA[Going to IndieWeb Summit 2018]]></title><description><![CDATA[<p>Once again, I'll be <data class="p-rsvp" value="yes">attending</data> the <a href="https://2018.indieweb.org/" class="u-in-reply-to">IndieWeb Summit</a> this year. Probably I'll work on <a href="https://github.com/strugee/lazymention">lazymention</a> and the <a href="https://github.com/strugee/strugee.github.com/tree/social-stream"><code>social-stream</code> branch</a> of this website. Maybe I'll work on <a href="https://stratic.js.org/">Stratic</a> too! I'm super excited.</p>
]]></description><link>https://strugee.net/blog/2018/05/going-to-indieweb-summit-2018</link><guid isPermaLink="true">https://strugee.net/blog/2018/05/going-to-indieweb-summit-2018</guid><category><![CDATA[personal]]></category><pubDate>Wed, 23 May 2018 18:14:52 GMT</pubDate></item><item><title><![CDATA[pump.io 5.1.1, Docker images, and sunsetting Node 4 support]]></title><description><![CDATA[<p>It's been a (relatively) long time since we've put anything on this blog, and I think it's high time for an update - especially since there are so many exciting things afoot! Not only is pump.io 5.1.1 now on npm, but we have new experimental Docker images! With <a href="https://medium.com/the-node-js-collection/april-2018-release-updates-from-the-node-js-project-71687e1f7742">upstream having already dropped security support</a>, we're also planning to drop support for Node 4 soon.</p>
<p>Let's take these one at a time.</p>
<h1>pump.io 5.1.1</h1>
<p>Several months ago I <a href="https://github.com/pump-io/pump.io/pull/1438">landed</a> a patch from contributor <a href="https://github.com/vxcamiloxv">Camilo QS</a> fixing a bug in pump.io's session handling in a route serving uploads. This bug made it so that non-public uploads would always return HTTP 403 Unauthorized, even if the user actually <em>was</em> authorized. Clearly, this makes uploads unusable for people who don't like to post everything publicly. <a href="https://identi.ca/evan">Evan</a> suggested that we should backport this bugfix since it's so high-impact, and I agree. So that's what pump.io 5.1.1 contains: a bugfix for uploads. Since it's a patch release 5.1.1 is a drop-in replacement for any 5.x pump.io release, so I'd highly encourage administrators to upgrade as soon as it's convenient. We'd also love it if you <a href="https://github.com/pump-io/pump.io/issues">file any bugs you find</a>, and feel free to get in touch with the <a href="https://github.com/pump-io/pump.io/wiki/Community">community</a> if you need help or have questions. As a reminder, you can subscribe to our <a href="https://lists.strugee.net/mailman/listinfo/pumpio-announce">low-volume announce mailing list</a> to get email when we put out new releases like this. Also, I would be remiss if I didn't mention that my signing key setup has changed temporarily - see <a href="https://strugee.net/blog/2018/04/new-temporary-signing-keys">here</a> if you want to cryptographically verify the 5.1.1 release.</p>
<p>If you're on an npm-based install, you can upgrade with <code>npm install -g pump.io@5.1.1</code>. If you're on a source-based install, you can upgrade by integrating the latest commits in the <code>5.1.x</code> branch. See <a href="https://github.com/pump-io/pump.io/blob/master/CHANGELOG.md#511---2018-05-05">here</a> for the changelog.</p>
<p>But that's not all. pump.io 5.1.1 also includes another exciting change: with this release, we've integrated automation to relase pump.io Docker images too.</p>
<h1>Docker images</h1>
<p>We've wanted to release pump.io Docker images for <a href="https://github.com/pump-io/pump.io/issues/789">a long time</a>. But Docker has a well-known problem: security vulnerabilities in Docker Hub images <a href="https://www.infoq.com/news/2015/05/Docker-Image-Vulnerabilities">are</a> <a href="https://blog.acolyer.org/2017/04/03/a-study-of-security-vulnerabilities-on-docker-hub/">rampant</a>. Even though we've had a <code>Dockerfile</code> in the repository <a href="https://github.com/pump-io/pump.io/pull/1348">for a while</a> thanks to contributor <a href="https://github.com/JanKoppe">thunfisch</a>, we didn't want to release official Docker images if we weren't sure we could always provide security support for them.</p>
<p>Unfortunately, Docker the company has done very little to address this problem. Most of their solutions are aimed at image consumers, not authors. Docker Hub has <em>some</em> capacity for automatically rebuilding images, but unfortunately, it's not enough and you end up having to roll everything yourself anwyay. Pretty disappointing - so we had to get creative.</p>
<p>Our solution to this problem is to utilize Travis CI's <a href="https://docs.travis-ci.com/user/cron-jobs/">cron functionality</a>. Every day, Travis will automatically trigger jobs that do nothing but build pump.io Docker images. These images are then pushed to Docker Hub. If nothing has changed, Docker Hub recognizes that the "new" images are actually identical with what's already there, and nothing happens. But if there <em>has</em> been a change, like a native dependency receiving a security update, then the image ID will change and Docker Hub will accept the updated image. This cronjob is enabled for the <code>5.1.x</code> branch and master (which as a side effect, means that alpha Docker images are published within 24 hours of a git push), and in the future it will be enabled on all branches that we actively support. Thus, Docker users can easily set up automation to ensure that they're running insecure images for, at most, 24 hours.</p>
<p>If you're interested in trying out the Docker images, we'd love to know how it goes. They should still be treated as experimental at the moment, and early feedback would be super useful. You can read more details in our <a href="https://pumpio.readthedocs.io/en/latest/installation/about-docker-images.html">ReadTheDocs documentation</a>.</p>
<p>Note that there are still more changes that we'd like to make to the Docker images. These changes didn't make it into the 5.1.1 packaging since they felt too invasive for a patch release. Instead we plan to make them in the next release, which is planned to be semver-major. Which brings me neatly to the last topic...</p>
<h1>Sunsetting Node 4, 5, and 7 support</h1>
<p>We had a good run, but it's time to say goodbye: Node.js upstream has <a href="https://medium.com/the-node-js-collection/april-2018-release-updates-from-the-node-js-project-71687e1f7742">marked</a> Node 4.x as end-of-life, and in accordance with our <a href="https://github.com/pump-io/pump.io/wiki/Node.js-version-support">version policy</a>, we're doing the same. Since this is a semver-major change, we're also taking the opportunity to drop support for Node 5.x and Node 7.x. These changes have been made as of commit <a href="https://github.com/pump-io/pump.io/commit/32ad78812ed767621418b8dd57f11ce86a01b49f">32ad78</a>, and soon we'll be ripping out old code used to support these versions, as well as upgrading dependencies that have recently started requiring newer Nodes.</p>
<p>Anyone still on these versions is encouraged to upgrade as soon as possible, as Node.js upstream is no longer providing security support for them. Administrators can use the <a href="https://github.com/nodesource/distributions">NodeSource</a> packages, or they can try out our new Docker images, which use a modern Node version internally.</p>
<p>Please reach out to the <a href="https://github.com/pump-io/pump.io/wiki/Community">community</a> if you need any help making the transition. And good luck!</p>
]]></description><link>https://strugee.net/blog/2018/05/pump.io-5.1.1-docker-images-and-node-4</link><guid isPermaLink="true">https://strugee.net/blog/2018/05/pump.io-5.1.1-docker-images-and-node-4</guid><category><![CDATA[pump.io]]></category><category><![CDATA[releases]]></category><category><![CDATA[development]]></category><pubDate>Sat, 05 May 2018 20:06:18 GMT</pubDate></item><item><title><![CDATA[New temporary signing keys]]></title><description><![CDATA[<p>So unfortunately, recently I have lost my Nitrokey, which I liked very much. In addition to this being fairly upsetting, I am now left with the sticky situation of not being able to sign code - while I have a master key (from which I can generate new subkeys), I'm currently at college and my copy of the key is sitting 3,000 miles away at home.</p>
<p>To get around this situation, I've generated a temporary signing keypair. This keypair is set to expire after 3 months (and I don't intend to renew it). When I have access to my master keypair, I will revoke the old subkeys, generate new subkeys (it was probably time, anyway) and revoke the temporary keypair.</p>
<p>The new fingerprint is <code>D825FD54D9B940FF0FFFB31AA4FDB7BE12F63EC3</code>. I have uploaded the key to <a href="https://github.com/strugee.gpg">GitHub</a> as well as the Ubuntu keyserver and <code>keys.gnupg.net</code> (just as my original was). The key is also incorporated into my Keybase account so that you can bootstrap trust in it, if you want to verify software signatures or whatever.</p>
]]></description><link>https://strugee.net/blog/2018/04/new-temporary-signing-keys</link><guid isPermaLink="true">https://strugee.net/blog/2018/04/new-temporary-signing-keys</guid><category><![CDATA[personal]]></category><category><![CDATA[security]]></category><category><![CDATA[blaggregator]]></category><pubDate>Tue, 17 Apr 2018 23:01:33 GMT</pubDate></item><item><title><![CDATA[How to evaluate domain registrars: a DNS story]]></title><description><![CDATA[<blockquote>
<p><em>Adriel</em>: Any recommendations of where I should buy domain names from?</p>
<p><em>Me</em>: I've heard gandi.net is good and they support free software projects and organizations - I'm on Namecheap at the moment but it's janky and sometimes unreliable, and basically the only reason I'm still with them is inertia</p>
<p><em>Adriel</em>: What makes the difference? Why does it matter where it comes from? I guess a better question is, what am I actually buying when I buy a domain?</p>
<p><em>Narrator</em>: DNS is fraught with peril and complexity, and many hours would go by before Adriel got his answer...</p>
</blockquote>
<p>This is a cleaned-up version of an explanation of how the DNS works and what owning a domain name really means. The original was much worse, because I typed it on my phone into a WhatsApp group chat and didn't edit at all. But you, dear reader, get the new-and-improved version! I'll start with <a href="https://github.com/aarsenault">Adriel</a>'s original question - why does it matter where my domain name comes from - and then transition into talking about how the DNS works and how the domain registar plays into that picture.</p>
<p>Here is, in my <em>highly</em>-opinionated opinion, the golden rule of registrar shopping - the #1 thing you have to know: <em>all registrars are sketchy. Therefore you are looking for the least terrible option.</em></p>
<p>Once you understand this, you're looking at four things registrars have to provide:</p>
<ol>
<li>DNS services</li>
<li>Whois data</li>
<li>The web UI for managing everything</li>
<li>Support</li>
</ol>
<p>These are the most important, but depending on your usecase you might also want to examine value-add services registrars provide. Most registrars will also host your email and provide shared hosting, or sometimes a virtual private server, or VPS. A VPS is a box where you get root and can do whatever you want, including hosting a web server (but you have to do it yourself); shared hosting is where you get a managed web server installation that's shared (get it?) with other people. You get write access to a particular
directory on the box that the web server is configured to serve from. (Registrars often also provide TLS/HTTPS certificates, but now that <a href="https://letsencrypt.org/">Let's Encrypt</a> exists, why you'd pay for one unless you need an <a href="https://en.wikipedia.org/wiki/Extended_Validation_Certificate">EV cert</a> is beyond me.)</p>
<p>The third and fourth responsibilities are pretty easy to understand. Is the web UI butt-ugly or buggy, and is the support staff friendly and responsive. So I want to focus on the first responsibility, DNS, because that can be super confusing (I don't really understand the second, Whois, and anyway this post is long enough already). Here's the tl;dr: the registrar provides the servers that are responsible for answering DNS queries. Even if you use a third-party provider your registrar is involved, because they have to serve <code>NS</code> records that basically say "hey look over at this other provider for the REAL records."</p>
<p>Let's break down exactly what that means. Before I start I should note that if you've ever heard people say something along the lines of, "it'll take up to 24 hours for DNS to propogate," you should forget that. It's a convenient lie that people (including myself sometimes!) use to explain DNS' caching behavior more easily.</p>
<p>DNS works by recursively resolving each component in a domain name. It's probably easiest to demonstrate how this works by walking through the steps clients like browsers take when they're trying to resolve a domain name into a numeric IP address. So say we're trying to resolve <code>example.com</code>, with no help from anybody else.</p>
<p>Our first step is to look up the authoritative nameservers for the <code>com</code> component. In other words, we're looking up the servers that have the absolute final word as to what DNS records <code>com</code> has. (More on exactly how this first step works later.) Once we've found the <code>com</code> nameservers, we issue a DNS query asking them where we can find the nameservers for <code>example.com</code>. The answer we get back will point to <code>example.com</code>'s registrar. Always. Even if they're using a different DNS service - the registar just points to the other service with <code>NS</code> records.</p>
<p>Let's pause in our journey to resolve <code>example.com</code> for a second to consider the implications of this. This means that the registrar is <strong>always</strong> involved in DNS lookups for a domain, which is important for two reasons:</p>
<ol>
<li>If the registrar's DNS goes down so does your website</li>
<li>If you want to use DNSSEC on your domain (which of course I have Opinionsâ¢ on but whatever) your registrar has to support it, because the way DNSSEC works is that EVERY link in the lookup chain MUST be signed, or the entire chain of trust falls apart[1]</li>
</ol>
<p>Just things to bear in mind.</p>
<p>Anyway, we're almost done resolving <code>example.com</code>. We've found its nameserver, so all we have to do is issue our original query. Maybe we need an IPv4 address, in which case we'd ask the nameserver for all <code>A</code> records for <code>example.com</code>; maybe we want IPv6 addresses instead in which case we'd ask for <code>AAAA</code> records. (If you want to know more about DNS record types, <a href="https://en.wikipedia.org/wiki/List_of_DNS_record_types">Wikipedia has a nice list</a>!) If the registrar is the authoritative nameserver for <code>example.com</code> it'll respond normally, if <code>example.com</code> uses a 3rd-party DNS host, the registrar will respond with <code>NS</code> records. In the former case, we've gotten the data we originally set out to get; in the latter, we simply issue our query again to the nameserver the registrar pointed us at - which we now know to be authoritative - and if all goes well, we'll get a regular response and be done. As a reminder, the "authoritative nameserver" for a given domain is whatever nameserver contains the authoritative data <strong>for that domain</strong>. So we say e.g. "such-and-such a registrar's nameservers are <em>authoritative for</em> <code>example.com</code>." For <code>example.net</code> the authoritative nameservers could be completely different.</p>
<p>The overarching goal of the recursive resolution procedure I just laid out is simply to find that veeery last nameserver in the chain which can speak authoritatively for the domain we're interested in. Your registar's job, as far as DNS is concerned, is to put the domain in the nameservers for the top-level domain (or TLD - <code>com</code> in <code>example.com</code>'s case) and to either serve regular DNS records or point recursive DNS resolvers at the authoritative nameserver that will. There's also some other paperwork involved I think, but I wouldn't know much about that.</p>
<p>As an aside, I should note that normally, your computer doesn't do all this recursive stuff. There will be some DNS server upstream - perhaps running on your router or run by your ISP - that does it for you. This is to alleviate your computer from having to know how to do this and also because it makes stuff like caching work better. Speaking of caching, let's talk about the <em>real</em> explanation behind the admittedly super-convenient "24 hour propogation" lie. Now that we know how DNS resolution works the idea of "DNS propogation" is pretty simple to understand - it comes from caching. All that recursion stuff is expensive in terms of time (and nameserver load), so we want to avoid doing it whenever possible by generating responses from a local cache. This is accomplished by a Time To Live (TTL) associated with every DNS record response. The TTL basically says how long the record is valid for, and therefore how long it can be cached. When you change your DNS records and wait for it to "propogate", really you're just waiting for caches to expire. That means, though, that if you plan ahead you can lower your TTL to a few seconds, wait until everybody's cache expires (so everyone sees the new TTL), and then make your change[2]. This would lower downtime to a minimum. To be polite you'd then raise your TTL again. If you want to know more about this, <a href="https://serverfault.com/a/125378/167999">here</a> is an excellent Server Fault question that rehashes this explanation and then describes an exponential backoff strategy you might use to make such a change as efficiently as possible.</p>
<p>And with that, we've covered most of what you need to know about how DNS works, except for one thing - that mysterious first step. How did we get the nameservers for <code>com</code>?</p>
<p>The answer is the <a href="https://en.wikipedia.org/wiki/Root_name_server">DNS root</a>. The DNS root servers are at the absolute top of the DNS hierarchy, and they're responsible for resolving TLDs. In fact, if I was to nitpick, I'd point out that I lied earlier when I said we were trying to resolve <code>example.com</code>. In reality, we're trying to resolve <code>example.com.</code> (note the trailing <code>.</code>). If we read this new domain name, which is now <em>fully qualified</em>, from right-to-left, we get the chain we used to lookup the final authoritative nameserver. It goes: empty string, indicating the root "zone" (to use DNS parlance) -&gt; <code>com</code> -&gt; <code>example</code>. DNS is technically distributed but organizationally highly centralized around the DNS root, and trust in the DNS root, and people who don't like that tend to run alternative DNS roots (your friendly author is sympathetic to this position). But that's a story for another time.</p>
<p>Hopefully you now have a much better idea of how the DNS works and why your registrar has to be involved in it. I'm 95% sure this post is accurate, but I could always be wrong so if you spot a mistake, please feel free to contact me either by <a href="https://strugee.net/contact">the usual ways</a> or by Zulip if you're a Recurser. Good luck!</p>
<p>Footnotes:</p>
<p> [1]: FWIW this is how Namecheap screwed up my domain - I turned on their DNSSEC option, but they had messed up their signing so that suddenly any resolver which validated DNSSEC would reject the legitimate responses as invalid. This was made extremely difficult to debug by the fact that you don't really know if a resolver is doing this for you, and often if DNSSEC validation fails that failure won't be passed directly on to the client (the client instead will get <code>NXDOMAIN</code> or something, which is basically the DNS way of saying "there's nothing there"). So resolution would fail on half the networks I tested from, with no clear difference. To make matters worse, when I went to turn of Namecheap's DNSSEC support because my website was down for a basically semi-random sampling of my visitors, the change didn't actually propogate to production! Like, I flipped the switch in the admin panel and it literally did <em>nothing</em>. So I had to escalate a support ticket to get them to purge all the DNSSEC stuff from production. Kinda ridiculous!</p>
<p> [2]: do note, however, that <a href="https://serverfault.com/a/740637/167999">people sometimes don't follow the standard</a> and set a lower bound (i.e. minimum) on TTLs. So if you make a change make sure stuff running on your old IP address won't wreak havoc if it gets production traffic.</p>
]]></description><link>https://strugee.net/blog/2018/02/how-dns-works</link><guid isPermaLink="true">https://strugee.net/blog/2018/02/how-dns-works</guid><category><![CDATA[explanations]]></category><category><![CDATA[blaggregator]]></category><pubDate>Thu, 01 Feb 2018 00:00:42 GMT</pubDate></item><item><title><![CDATA[Winter break retrospective & spring semester goals]]></title><description><![CDATA[<p>Tonight I'll have been back at college for a full week, and I wanted to write up a little retrospective of winter break to see what I accomplished (and in particular, which <a href="https://strugee.net/blog/2017/12/winter-break-priorities-2017-18">goals</a> I completed or missed).</p>
<p>You may wish to skip directly to the <a href="https://strugee.net/blog/#executive-summary">executive summary</a>.</p>
<h1>Resolved goal: Node.js manpage PR</h1>
<p>I didn't <em>complete</em> this goal per se, but I did at least resolve it <a href="https://github.com/nodejs/node/pull/14164#issuecomment-357909699">by closing the Pull Request</a>. I felt pretty bad about it (especially because I kept saying I intended to finish it) but honestly, it became clear to me that I'd just lost the motivation to keep going with it. I would love it if this was included in Node.js core, but I just consistently have higher priorities. So rather than leave it hanging and cluttering up the Pull Requests view, I just closed it to reflect reality. I made sure not to delete the branch though, in case someone (distant future me?) wants to pick it up again.</p>
<h1>Failed goal: deal with GPG keysigning</h1>
<p>I did nothing to push this goal forward. While I made <a href="https://strugee.net/blog/2018/01/improving-gpg-security">numerous improvements to my GPG setup</a>, I did not actually sign anyone's key, which was what this goal was about. This feels unfortunate. (I also do not have access to the private key material in college, and am <em>certainly</em> not about to ask that it be shipped to me.)</p>
<h1>Partially completed goal: push Debian packaging work forward</h1>
<p>There were two components to this: <a href="https://tracker.debian.org/pkg/profanity">Profanity packaging</a> upgrades and getting the new <a href="https://github.com/strugee/filter-other-days">filter-other-days</a> packaging into Debian. I made no progress on the Profanity packaging. However, I did <a href="https://github.com/strugee/dots/commit/2b477a7079de9ff675fd4e2d22f58938ffbb7bc9">fix a misconfiguration in my <code>.reportbugrc</code></a> (which annoyingly had previously sent my incredibly detailed email about Profanity packaging to <code>/dev/null</code>) and then submitted <a href="https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=886310">an ITP</a> (Intent To Package bug, which is a Debian thing) for filter-other-days. I then used that ITP bug number <a href="https://github.com/strugee/filter-other-days/commit/d09943c6ac4eea07720cc88596bed3594c3f4644">to fix the last <code>.deb</code> Lintian warning</a> (although see below). Then I paired with <a href="https://github.com/anjakefala">Anja</a> who is, as always, an angel, and we figured out the weirdness that is <code>dput</code> and <a href="https://mentors.debian.net/">mentors.debian.net</a>. Finally I was able to upload filter-other-days(!) BUT I was in for a rude awakening - apparently Lintian checks for <code>.deb</code>s and <code>.dsc</code>s are different. So while my binary package was Lintian-clean, my source package unfortunately wasn't. This is something I will need to work on in the weeks to come. That being said, I'm still pretty proud of what I've accomplished here! I've made significant progress on this front.</p>
<h1>Completed goal: lazymention v1</h1>
<p>One of the first things I did was ship lazymention 1.0.0 - and I wrote <a href="https://strugee.net/blog/2017/12/announcing-lazymention-elegant-outbound-webmention-for-static-sites">an announcement blog post</a> to accompany it! (In fact, I syndicated that blog post to <a href="https://news.indieweb.org/">IndieNews</a> <em>using lazymention</em>, which felt pretty damn awesome.) I got some great feedback on it from the IndieWeb community, and my <a href="https://lobste.rs/s/kip3yk/announcing_lazymention_elegant">lobste.rs submission</a> - which also got some great engagement - even made the front page, which was pretty unreal! I still have a lot more work to do with lazymention - in particular, it doesn't actually respect edits (so it'll resend Webmentions with every job submission) - but for now it works well. I'm super pleased with it, and have integrated it into my site deployment workflow. I even wrote <a href="https://github.com/strugee/ping-lazymention/">a module</a> so other people can do this, too!</p>
<h1>Failed goals: ActivityPub in pump.io core, pump.io PR review</h1>
<p>No progress on this front. I did start hacking on a <a href="https://github.com/pump-io/telemetry">telemetry server</a> which will eventually be helpful for our ActivityPub rollout, but that did not in any way <em>directly</em> help fulfill these goals. I also <a href="https://strugee.net/blog/2018/01/pump.io-5.1-stable-published-to-npm">released 5.1 stable</a>, but that's pretty routine by this point.</p>
<h1>Partially completed goal: two blog posts per week</h1>
<p>I stuck with this goal all the way up until the final week, where I didn't write any. (Although I wrote about my <a href="https://strugee.net/blog/2018/01/improving-gpg-security">GPG keys</a> around the time I actually flew back to college.) The first week, I wrote about <a href="https://strugee.net/blog/2017/12/shell-script-is-one-of-the-purest-forms-of-human-expression">my thoughts on shell script</a> and about <a href="https://strugee.net/blog/2017/12/announcing-lazymention-elegant-outbound-webmention-for-static-sites">lazymention</a>; the second, I wrote about the <a href="https://strugee.net/blog/2018/01/pump.io-5.1-stable-published-to-npm">pump.io 5.1 stable release</a> and about <a href="https://strugee.net/blog/2018/01/tell-your-pr-reviewers-theyre-wrong">talking to Pull Request reviewers if you think they're wrong</a>.</p>
<h1>Failed stretch goal: paper editing</h1>
<p>I did absolutely no editing on the paper I intend to get published (which I originally wrote for a writing class). This was a stretch goal though, so that's totally fine.</p>
<h1>Additional activity: steevie maintenance</h1>
<p>After I finally found the cable I needed, I swapped out the cable that connects steevie's motherboard with the drives' SATA ports. This seemed to significantly improve disk performance, although there are still noticeable performance problems. I'm very, <em>very</em> happy to have finally gotten this done.</p>
<h1>Additional activity: Tor relay migration from Amazon EC2 to DigitalOcean</h1>
<p>After getting some advice on <a href="https://lists.torproject.org/cgi-bin/mailman/listinfo/tor-relays">tor-relays</a>, I <em>finally</em> sat down and looked into moving my relay away from Amazon Web Services. This is because AWS bills by usage, which for a Tor relay ends up being incredibly inefficient. It turned out to actually be way easier than I thought, which only served to make me mad that I hadn't done it sooner. In any case, I now save approximately $240/year AND I can push 1000GB/month as opposed to the 10GB/month I pushed before. In the words of <a href="https://github.com/strugee/torrc/commit/8b9fe85378adc834b8b7a9953de45f508b76bc3e">the commit where I made this change</a>: "this change made possible by the fact that I'm no longer getting billed up the wazoo by Amazon Web Services." Here's a of <a href="https://atlas.torproject.org/#details/C3CFCC9B5993A6F0D1349858C598C4A78AFE51F9">a Tor Metrics graph</a> (captured today) that shows the jump:</p>
<p><img src="/images/tor-relay-graph.svg" alt="Tor Metrics graph"></p>
<p>Anyway, I'm super happy I can contribute more to the Tor network <em>and</em> save lots of money in the process. That being said I am pretty damn salty I didn't realize this in the four <em>years</em> I've been running a Tor relay.</p>
<h1>Additional activity: offandonagain.org maintenance</h1>
<p>After turning on <a href="https://nodesecurity.io/">NodeSecurity</a> monitoring for <a href="https://offandonagain.org">offandonagain.org</a>, I found out that the module that underlies it, <a href="https://github.com/strugee/node-serve-random">node-serve-random</a>, had some vulnerable dependencies. Not only did I fix this, I wrote a large test suite for the module and found (and fixed!) <a href="https://github.com/strugee/node-serve-random/blob/master/CHANGELOG.md#200---2018-01-13">several bugs</a> in the process. Writing a test suite also allowed me to turn on <a href="https://greenkeeper.io/">Greenkeeper</a> for the module, which will be a huge help to me in keeping the module up-to-date.</p>
<h1>Additional activity: Stratic work</h1>
<p>First off, I released <a href="https://github.com/straticjs/generator-stratic/blob/master/CHANGELOG.md#100-beta-7---2017-12-3">beta 7</a> of <a href="https://github.com/straticjs/generator-stratic">generator-stratic</a>! Nothing major, just some polishing stuff. Stratic is getting very close to the point where I might want to start promoting it more aggressively, or declaring it stable, and (with a <em>lot</em> of super-helpful feedback from my family) I worked on something that's super important before we get there: <a href="https://github.com/straticjs/RFCs/issues/22">a logo</a>!</p>
<p>Here are two of my favorites so far:</p>
<div class="two-image-container">
<p><img src="/images/stratic-logo-asteroid-with-pipe.svg" alt="Yellow background with a centered black file icon and a asteroid coming up from earth in the midddle and a pipe to the right"> <img src="/images/stratic-logo-rocket-with-pipe.svg" alt="Yellow background with a centered black file icon and a rocket coming up from earth in the midddle and a pipe to the right"></p>
</div>
<p>These are based off the JS logo, in case you hadn't seen it before:</p>
<p><img src="/images/js-logo.svg" alt="Black JS text in the bottom-right corner of a yellow background"></p>
<p>Anyway, I have to post another iteration in the <a href="https://github.com/straticjs/RFCs/issues/22">GitHub issue</a> based on some feedback from <a href="http://saul.pw/">Saul</a> (who I had a lovely lunch with) - he thinks I should reverse it so the pipe is on the left, so it looks like the file is coming out of the pipe. But anyway you should comment there if you have feedback!</p>
<h1>Additional activity: IndieWeb stuff</h1>
<p>I attended Homebrew Website Club in San Fransisco, which was <em>incredible</em>. I got to meet a bunch of new people, as well as say hi to <a href="http://snarfed.org/">Ryan</a> and <a href="http://tantek.com/">Tantek</a> again, which was so nice - it's always just <em>better</em> to talk in real life. Tantek said (at least if I recall correctly) that my laptop was one of the best-stickered laptops he'd ever seen, which made me feel just unbelievably special. He also gave me a <a href="http://microformats.org/">microformats</a> sticker (and helped me figure out where to put it), which I had on my old laptop and had been really missing, as well as a <a href="https://letsencrypt.org/">Let's Encrypt</a> sticker. The latter was so special I elected to put it on the inside of my laptop, which I reserve only for <em>really</em> special things (basically a <a href="https://www.recurse.com/">Recurse Center</a> refucktoring sticker and a sticker of <a href="https://github.com/SwartzCr">Noah</a> in <a href="https://www.eff.org/encrypt-the-web">this video</a>, which he handed to me like a business card the first time we met). Anyway, every time I look at the Let's Encrypt sticker I just feel so happy. I love Let's Encrypt so damn much.</p>
<p>Homebrew Website Club was super inspiring, so when I got back to where I was staying (at my mom's boyfriend's house) I started implementing an <a href="https://indieweb.org">IndieWeb</a>-style social stream for strugee.net. It still needs some polishing but is actually pretty close to being done. Who knows when I'll have time to finish it, but it's getting there! I'm so freaking excited, honestly. Also, I added proper timestamp mf2 metadata to my site, as well as a visual display for post edits, and I expanded what type of Webmentions the display code recognizes too!</p>
<!-- TODO nuke this when I actually do proper header anchors -->
<p><span id="executive-summary"></span></p>
<h1>Executive summary</h1>
<p>I resolved or completed 2 goals, partially completed 2 goals, failed 3 goals, and failed 1 stretch goal. Additionally I did significant work in 5 other areas. Out of the goals I set for myself, I completed 51% (Debian packaging work is ~2/5 complete; blog posts were written 2/3 of the time); not counting the stretch goal, I completed 61.2%. I'm pretty happy with what I got done during this period; however, while I was productive, the numbers show that I did a mediocre job sticking to my goals. In the future I should focus on making more realistic goals and then sticking to them (though not too much - it is a break, after all!).</p>
<p>Speaking of which, partway through break I felt like I was on the edge of burnout, which to me was a <em>very</em> clear sign that I was pushing myself way too hard during a time I should have been unwinding. Because of that I cut what I was doing a <em>lot</em>, which helped pretty dramatically. In fact, I think without that I wouldn't have been able to do some of the later stuff, like all the IndieWeb work. So that's another reason I have to find a way to balance sticking to goals and just relaxing (which doesn't necessarily mean not coding, just doing whatever coding I feel like in the moment) - I feel like I was pushing myself too hard to meet my goals (and then getting distracted and not meeting them) and that's what led to the feeling. Obviously there are different constraints for e.g. schoolwork; here I'm referring <em>only</em> to free time like breaks.</p>
<h1>Spring semester goals</h1>
<p>With that in mind, I want to set some very broad goals for spring semester. I may update this list as time goes on, but for now I have four overarching goals I want to accomplish (besides the usual day-to-day code maintenance stuff):</p>
<ul>
<li>Finish editing the paper I wrote last semester on freedom-respecting software and intersectionality, and get it published</li>
<li>Make <em>some</em> measurable progress on my <a href="https://github.com/strugee/draft-webpush-2fa">Push-based Two-factor Authentication IETF draft</a></li>
<li>Get access to the University of Rochester darkroom and start developing/printing photos again</li>
<li>Start pushing the University of Rochester library (and <em>maybe</em> the journalism department?) to start adopting Tor technologies</li>
</ul>
<p>I'm excited to see how it goes!</p>
]]></description><link>https://strugee.net/blog/2018/01/winter-break-retrospective</link><guid isPermaLink="true">https://strugee.net/blog/2018/01/winter-break-retrospective</guid><category><![CDATA[personal]]></category><category><![CDATA[development]]></category><category><![CDATA[blaggregator]]></category><pubDate>Tue, 23 Jan 2018 20:51:03 GMT</pubDate></item><item><title><![CDATA[Improving GPG security]]></title><description><![CDATA[<p>Recently I've been putting some effort into improving the security of my <a href="https://strugee.net/gpg">GPG key</a> setup, and I thought I would take a moment to document it since I'm really excited!</p>
<h1>Nitrokey</h1>
<p>First, and most importantly, I have recently acquired a <a href="https://www.nitrokey.com/">Nitrokey Storage</a>. After I initialized the internal storage keys (which took a <em>really</em> long time), I used <code>gpg --edit-key</code> to edit my local keyring. I selected my first subkey, since in my day-to-day keyring the master key's private component is stripped, and issued <code>keytocard</code> to move the subkey to the Nitrokey. Then I repeated the process for the other subkey.</p>
<p>In the middle of this I <em>did</em> run into an annoying issue: GPG was giving me errors about not having a pinentry, even though the <code>pinentry-curses</code> and <code>pinentry-gnome3</code> packages were <em>clearly</em> installed. I had been dealing with this issue pretty much since I set up the system, and I had been working around it by issuing <code>echo "test" | gpg2 --pinentry-mode loopback --clearsign &gt; /dev/null</code> every time I wanted to perform a key operation. This worked because I was forcing GPG to not use the system pinentry program and instead just prompt directly on the local TTY; since I had put in the password, <code>gpg-agent</code> would then have the password cached for a while so I could do the key operation without GPG needing to prompt for a password (and thus without the pinentry error). However, this didn't seem to work for <code>--edit-key</code>, which I found supremely annoying.</p>
<p>However this turned out to be a good thing because it forced me to <em>finally</em> deal with the issue. I tried lots of things in an effort to figure out what was going on: I ran <code>dpkg-reconfigure pinentry-gnome3</code>, <code>dpkg-reconfigure gnupg2</code>, and I even manually ran <code>/usr/bin/pinentry</code> to make sure it was working. Turns out that, like many helpful protocols, the pinentry protocol lets you send <code>HELP</code>, and if you do so you'll get back a really nice list of possible commands. I played around with this and was able to get GNOME Shell to prompt me for a password, which was then echoed back to me in the terminal!</p>
<p>Despite feeling cool because of that, I still had the pinentry problem. So finally I just started searching all the GPG manpages for mentions of "pinentry". I looked at <code>gpg(1)</code> first, which was unhelpful, and then I looked at <code>gpgconf(1)</code>. That one was also <em>mostly</em> unhelpful, but the "SEE ALSO" section <em>did</em> make me think to look at <code>gpg-agent(1)</code>, where I hit upon the solution. Turns out <code>gpg-agent(1)</code> has a note about pinentry programs right at the very top, in the "DESCRIPTION" section:</p>
<blockquote>
<p>Please make sure that a proper pinentry program has been installed under the default filename (which is system dependent) or use the option <code>pinentry-program</code> to specify the full name of that program.</p>
</blockquote>
<p>The mention of the <code>pinentry-program</code> option led me pretty immediately to my solution. I had originally copied my <code>.gnupg</code> directory from my old MacBook Pro, and apparently GPGTools - a Mac package that integrated GPG nicely with the environment (as well as providing a GUI I never used) - had added its own <code>pinentry-program</code> line to <code>gpg-agent.conf</code>. That line pointed at a path installed by GPGTools, which of course didn't exist on my new Linux system. As soon as I removed the line, <code>--edit-key</code> worked like a charm. (I've also just added <code>gpg-agent.conf</code> to my <a href="https://github.com/strugee/dots/blob/master/.gnupg/gpg-agent.conf">dotfiles</a> so I notice this kind of thing in the future.)</p>
<p>So far, I'm really enjoying my Nitrokey. It works really well and the app is pretty good, although the menu can be pretty glitchy sometimes. I've used the password manager for a couple high-security passwords (mostly bank passwords) which is great, and I've switched my two-factor authentication for GitHub from FreeOTP on my phone to the Nitrokey since GitHub is a super important account and I really want to make sure people can't push code as me.</p>
<p>There are only two problems I've had with the Nitrokey so far. The first is that it's slow. I notice a significant pause when I do any crypto operation, probably somewhere between a half a second to a second. This hits me quite often since I sign all my Git commits; however I suspect I'll get used to this, and the security benefits are well worth the wait anyway. The other problem is that the Nitrokey doesn't support FIDO U2F authentication. This wasn't a surprise (I knew it wouldn't when I was shopping models) but is nevertheless a problem I would like to deal with (which means getting a second device). The basic reason is just that U2F is newer than the Nitrokey I have. Other than those, though, I would highly recommend Nitrokey. The device is durable, too - I just carry it around in my pocket. (I briefly considered putting it on my keychain - for those of you who haven't met me in person, I have my keychain on an easily-detachable connector attached to a belt loop - but I decided against it because my keychain is kinda hefty.)</p>
<h1>Keybase</h1>
<p>In addition to the Nitrokey, I've also finally started using <a href="https://keybase.io/">Keybase</a>!</p>
<p>For a long time I wasn't too sure about Keybase. I felt like people should really be meeting in person and doing keysigning parties, and I didn't like that they encourage you to upload a private key to them, even if it's password-protected. Eventually I softened my position a little bit and got an invite from <a href="https://keybase.io/yawnbox">Christopher Sheats</a> (back then you needed an invite) but I only made it halfway through the install process before getting distracted and forgetting about it for, you know, several years.</p>
<p>This time, though, I decided to finally get my act together. Do I still kinda think it's a bummer that Keybase encourages private key uploads? Sure. Are real-life keysignings better? <em>Absolutely</em>. But even though they're better, a lot of experience trying to do them and teach them has thoroughly convinced me that they're just too impractical. There are lots of people who might need to at least have <em>some</em> trust in my key - for example, to verify software signatures - and this is a pretty decent solution for them. Not to mention a novel and interesting solution. Plus, it's possible to use Keybase in such a way that you're not compromising security in any way, which is the way I do it.</p>
<p>So tl;dr: I'm on the Keybase bandwagon now. <a href="https://keybase.io/strugee">My profile</a> is also now linked to from my <a href="https://strugee.net/gpg">GPG keys</a> page.</p>
<h1>Safe for master key</h1>
<p>Finally, my dad's wife's safe has recently been moved into our house and is conveniently sitting next to my computer. Currently, I keep my master key in a file on a flash drive with an encrypted LUKS container. When I need to access my master key, this file gets unlocked with <code>cryptsetup</code> and then mounted somewhere on my laptop, and I pass the <code>--homedir</code> option to <code>gpg</code> to point it at the mount location. This is better than just keeping the master lying around day-to-day, but still pretty unideal as I'm exposing it to a potentially compromised, non-airgapped computer. Therefore I plan to get a Raspberry Pi (or something similar) and put it in the safe so I can use it as a fully trusted computer that's never been connected to the internet (and is therefore <em>very</em> hard to compromise). I'll keep the Pi in the safe to provide greater assurance that it hasn't been tampered with, as well as to provide a physical level of redundancy for the key material's security. This will hopefully happen Real Soon Nowâ¢ - I can't wait!</p>
]]></description><link>https://strugee.net/blog/2018/01/improving-gpg-security</link><guid isPermaLink="true">https://strugee.net/blog/2018/01/improving-gpg-security</guid><category><![CDATA[security]]></category><category><![CDATA[personal]]></category><category><![CDATA[blaggregator]]></category><pubDate>Tue, 16 Jan 2018 23:01:13 GMT</pubDate></item><item><title><![CDATA[Tell your PR reviewers they're wrong]]></title><description><![CDATA[<p>Have you ever submitted a Pull Request and had the maintainer give you feedback that says something is wrong? This is of course, perfectly natural, and is why we do Pull Request reviews in the first place. But have you ever thought it was actually the <em>maintainer</em> doing the review who was wrong?</p>
<p>I'm the maintainer of a number of open source/free software projects and I have a message for you: <strong><em>tell this to the maintainer</em>, even if you're new, even if you feel like you have no idea what you're doing</strong>.</p>
<p>I'm sure to some people more experienced this sounds like obvious advice, but when you're new to this stuff getting feedback can be really scary. Maintainers have often spent years in the free software community and are super comfortable with how things work, not to mention that they might be more experienced in the language or framework their project uses. So it's super easy to just take their word as gospel. But I think it's super important to remember that everyone's human just like everyone else, and <em>everyone</em> has something to learn.</p>
<p>As a concrete example, take <a href="https://github.com/pump-io/pump.io/pull/1310#issuecomment-291554491">this comment</a> on a Pull Request I received. If you read it and the following comment, I seem like an expert on <code>eval()</code>, but what you don't see until you read all the way to the bottom is that I spent about 20 minutes composing that comment because I had to double-check MDN to make sure I was actually correct. And honestly, that was <em>great</em>. That Pull Request forced me to recheck what I thought about the way <code>eval()</code> worked, <em>and</em> the contributor learned from it too! So it ended up being a win-win. Even if I was wrong, that would have been great too, since I would've learned something new. Sometimes the thing being discussed is just a really tough or confusing problem, and getting feedback on a PR can be a really awesome chance for both you <em>and</em> the maintainer to collaborate more closely and figure it out together!</p>
<p>I'm sure this won't work in every community. But honestly, if it doesn't, the maintainer you're running into is probably an ass and not worth bothering with anyway. This is what doing things in the open is all about - <a href="https://www.gnu.org/philosophy/free-sw.html">freedom 1</a>, after all, is "the freedom to study the source code and make changes", the operative word being "study". The fact that knowledge is shared out in the open with anyone who wants it is one of the most amazing parts of this community, and I want to encourage you to stay curious and ask lots of questions.</p>
<p>So if you've ever walked away from a Pull Request review feeling like something wasn't quite right, consider this your permission slip to politely <em>tell your reviewer you think they're wrong</em>[1] - just make sure to be polite and explain why you think so! And remember, whatever they say, they're not criticizing you personally. I hope you embrace it as an opportunity to grow.</p>
<p>Footnotes:</p>
<p>[1]: Phrasing this as "I think you're wrong" instead of "you're wrong" is great for a lot of reasons, but one notable and less obvious one is that if the maintainer <em>does</em> end up being right, you won't feel silly.</p>
<p><em>Edited to clarify that I'm not advocating inpoliteness, to change the phrasing to "I</em> think <em>you're wrong", and to add some nice framing around a chance to collaborate on a difficult problem. Thanks to the Recurse Center folks whose feedback turned into these changes, particularly <a href="https://www.harihareswara.net/">Sumana Harihareswara</a> and <a href="https://jvns.ca/">Julia Evans</a>.</em></p>
]]></description><link>https://strugee.net/blog/2018/01/tell-your-pr-reviewers-theyre-wrong</link><guid isPermaLink="true">https://strugee.net/blog/2018/01/tell-your-pr-reviewers-theyre-wrong</guid><category><![CDATA[tips]]></category><category><![CDATA[culture]]></category><category><![CDATA[blaggregator]]></category><pubDate>Mon, 08 Jan 2018 03:04:55 GMT</pubDate></item><item><title><![CDATA[pump.io 5.1 stable published to npm]]></title><description><![CDATA[<p>Last night I officially published pump.io 5.1 to npm as a stable release!</p>
<p>As I wrote in the beta announcement, this release contains a variety of improvements:</p>
<ul>
<li><a href="https://strugee.net/blog/2017/08/zero-downtime-restarts-have-landed">Zero-downtime restarts</a>, which allows administrators to seamlessly roll over to new configurations and codebases</li>
<li>The daemon now generates startup log warnings on bad configurations, including insecure <code>secret</code> values and internal parameters</li>
<li>An official <code>Dockerfile</code> is now included with the release</li>
<li>The logged-out mobile homepage's menu icon is no longer incorrectly styled as black</li>
<li>An authorization problem with SockJS connections has been fixed</li>
</ul>
<p>5.1 stable <em>does</em> include one change the beta didn't: a bump to the version of the <code>gm</code> npm package which we depend on. This bump was done as a precautionary measure, as previous versions of <code>gm</code> depended on a version of the <code>debug</code> module which was vulnerable to denial-of-service security bugs.</p>
<p>As a project, we addressed these bugs <a href="https://strugee.net/blog/2017/10/denial-of-service-security-fixes-now-available">back in October</a> when we issued security releases for all supported release branches, and at the time we confirmed that the vulnerable function wasn't used by <code>gm</code>. Today's <code>gm</code> bump does <em>not</em> constitute a security release; instead, we're just bumping the version as a precautionary measure in case we missed something in October's assessment of the situation.</p>
<p>Aside from the <code>gm</code> bump, there are (as usual) miscellaneous version bumps included in this release. We've also started tracking test suite coverage information as well as overhauled our <a href="https://pumpio.readthedocs.io/">documentation on ReadTheDocs</a>, moving most of the in-repository documentation there.</p>
<p>If you want even more details of this release, you can also check out <a href="https://github.com/pump-io/pump.io/blob/master/CHANGELOG.md#510---2018-01-05">the changelog</a>.</p>
<p>pump 5.1 is a drop-in replacement for 5.0. That means if you're using our recommended installation method and installing from npm, you can upgrade with <code>npm install -g pump.io@5.1</code>. If you have a source-based install, you should merge and/or switch to the <code>v5.1.0</code> tag. And as always, if you encounter any problems, please feel free to reach out to the <a href="https://github.com/pump-io/pump.io/wiki/Community">community</a> or <a href="https://github.com/pump-io/pump.io/issues">file bugs you find</a>.</p>
<p>Finally, I would be remiss if I didn't point out that pump.io has a <strong>brand-new announcement mailing list</strong>! While the blog is great for announcing new releases, not everyone finds it convenient to check. Also, if we issue new betas in the middle of a release cycle, these aren't typically announced on the blog. Therefore in the future <em>all</em> new releases will be announced on the mailing list, not just initial betas. If you want to subscribe to the mailing list, you may do so <a href="https://lists.strugee.net/mailman/listinfo/pumpio-announce">here</a> - you'll get announcements of new features only, not e.g. feature announcements as seen on this blog. I hope people find this service useful!</p>
]]></description><link>https://strugee.net/blog/2018/01/pump.io-5.1-stable-published-to-npm</link><guid isPermaLink="true">https://strugee.net/blog/2018/01/pump.io-5.1-stable-published-to-npm</guid><category><![CDATA[pump.io]]></category><category><![CDATA[development]]></category><category><![CDATA[releases]]></category><category><![CDATA[blaggregator]]></category><pubDate>Sat, 06 Jan 2018 04:09:43 GMT</pubDate></item><item><title><![CDATA[Announcing lazymention: elegant outbound Webmention for static sites]]></title><description><![CDATA[<p>Last night I hit publish on version 1.0.0 of a new project, <a href="https://github.com/strugee/lazymention">lazymention</a>! Whoohoo!</p>
<p>tl;dr: lazymention exists to add Webmention support to static sites!</p>
<p>To elaborate a little bit, I developed lazymention because I had a problem with this site: I wanted to send outbound <a href="https://indieweb.org/Webmention">Webmentions</a> when I link to things, but my website is completely static. (Webmention, in case you didn't know, is a way to notify another website that you linked to them, so the other website can display some UI about your reply or whatever.) The page builds happen on my local machine, not on the server. One option would be to just send Webmentions from my local machine too, but this isn't really a good solution for a couple reasons. First, I couldn't do it automatically at build-time because the built pages wouldn't have been deployed to the server yet, so receivers of my Webmentions would reject the mentions due to the source being nonexistant. That meant that I would have to have a separate step, which wouldn't really be <em>that</em> big of a deal (lazymention requires pinging a server too) except for the second reason: I would need some way to keep track of where I'd already sent Webmentions to, and that would require synchronizing across computers. Probably the only decent way to do that would be to check it into Git, but having a program's data store checked in right next to the source code just feels kinda ugly. Plus, then it can't be shared with other people as a service.</p>
<p>So instead of doing it locally, I elected to build a server instead. Here's how it works: you mark up your stuff with <a href="https://indieweb.org/h-feed"><code>h-feed</code></a> and <a href="https://indieweb.org/h-entry"><code>h-entry</code></a>, and whenever anything happens (e.g. you publish a new blog post or whatever), you ping lazymention with the URL (either the feed or the post itself). lazymention will use your microformats2 markup to find the canonical location for a given post, then it will find all the links in the post and send Webmentions for them. And presto! You've just sent Webmentions for your blog. lazymention also records when it's sent mentions, so if you ping it again, nothing will happen unless you've updated your content. I'm also planning to add <a href="https://indieweb.org/WebSub">WebSub</a> support to lazymention, too, and that'll work in the exact same way.</p>
<p>lazymention is super easy to get started with, especially because I've provided thorough documentation in the <a href="https://github.com/strugee/lazymention/blob/master/README.md">README</a>. If you find anything that's confusing or missing, please let me know by <a href="https://github.com/strugee/lazymention/issues/new">filing an issue</a>! I'd love to get it fixed. In fact, I'd be thrilled to hear about both positive <em>and</em> negative installation experiences.</p>
<p>Oh, and one more thing - lazymention is reusable in other applications. If you're writing a Node.js app and want to reuse its HTTP API, you can use its embedding API to get at the Express application and <code>Router</code> used internally. I'm not sure if people will actually find this useful, but I wrote it just for kicks anyway. See <a href="https://github.com/strugee/lazymention/blob/master/README.md#embedding">the embedding documentation</a> for more!</p>
<p>Cheers, and happy mentioning! Elegant outbound Webmention for static sites is here.</p>
]]></description><link>https://strugee.net/blog/2017/12/announcing-lazymention-elegant-outbound-webmention-for-static-sites</link><guid isPermaLink="true">https://strugee.net/blog/2017/12/announcing-lazymention-elegant-outbound-webmention-for-static-sites</guid><category><![CDATA[development]]></category><category><![CDATA[indieweb]]></category><category><![CDATA[releases]]></category><category><![CDATA[blaggregator]]></category><pubDate>Fri, 29 Dec 2017 17:52:04 GMT</pubDate></item><item><title><![CDATA[Shell script is one of the purest forms of human expression]]></title><description><![CDATA[<p>While I was at the <a href="https://www.recurse.com/">Recurse Center</a>, I came up with an interesting theory, and today I thought I'd finally formally write about it on my blog. Here it is: shell script is one of the purest forms of human expression, at least as far as technology is concerned.</p>
<p>Why? Well, shell script is this weird mix of actual programming language things and macro expansions, and even the programming things tend to be... odd, to put it politely. As a trivial example, did you know that this:</p>
<pre><code>$ echo {1..10}
</code></pre>
<p>results in this output:</p>
<pre><code>1 2 3 4 5 6 7 8 9 10
</code></pre>
<p>Shell scripts[1] know how to expand number sequences; it's just built-in to them. Variables work with substitution too, and you can even build commands with them:</p>
<pre><code>PAGER=less
$PAGER longfile.txt
</code></pre>
<p>The key thing to note here is that the second line is replacing <code>$PAGER</code> with the literal text <code>less</code>, and then running the resulting command line. The "variable access" is basically just an inline macro expansion, and the shell doesn't really handle it very intelligently. (This is also why you often need to quote variables when you access them - if they have whitespace in them, the shell will interpret the variable as more than one word.)</p>
<p>Or take the type system of the shell: it has none[2]. For example, what does <code>0</code> mean? It could represent any one of these things:</p>
<ol>
<li>The number 0</li>
<li>The string "0"</li>
<li>A binary we're invoking (well, trying to invoke) called <code>0</code></li>
<li>An argument to a binary</li>
<li>File descriptor 0 (stdin)</li>
<li>A file named <code>0</code></li>
</ol>
<p>All of these things combine to make the shell incredibly flexible and powerful - you can often express a <em>lot</em> in shell script with very little characters by writing "macros" in the right way.</p>
<p>It's also what makes shell such a god-awful programming language.</p>
<p>Think about it - as programmers, much of what we do seeks to impose order on our systems. We lint our code and run it through style checkers. I personally like to turn up my linter settings to maximum. And we use the concept of types to help organize our code - sure, there's disagreement on how <em>much</em> order types should impose, but we all pretty much agree that there should be <em>some</em> concept of types and type safety. Hell, for the languages whose type systems are basically "wtf are types", we make tools to impose additional safety on top of that - take <a href="http://www.typescriptlang.org/">TypeScript</a> and <a href="https://flow.org/">Flow</a>, for example. We also love to (hate to) write tests to ensure that our code fulfills some interface or behavior contract. (Even the idea of an interface seeks to organize and compartmentalize complexity in the system.)</p>
<p>Humans are inherently messy. We're scatterbrained and easily distracted, and our thoughts (or at least, my thoughts) tend to jump all over the place. Shell script is an amazing way to express your ideas because it lets you get directly to the goal so quickly, in a way that matches that messiness that defines us as a species. But since so much of our work as programmers is about trying to counteract our messiness, shell script is a <em>terrible</em> way to write real programs, for the exact. Same. Reason.</p>
<p>Footnotes:</p>
<p> [1]: Those written in <code>bash</code>, at least.</p>
<p> [2]: Note that this is different from having a type system of "wtf are types?? <a href="https://www.destroyallsoftware.com/talks/wat">wat</a>" - JavaScript is a good example of a language with such a type system, as is (to my understanding) PHP. The shell, on the other hand, has no type system <em>at all</em>.</p>
]]></description><link>https://strugee.net/blog/2017/12/shell-script-is-one-of-the-purest-forms-of-human-expression</link><guid isPermaLink="true">https://strugee.net/blog/2017/12/shell-script-is-one-of-the-purest-forms-of-human-expression</guid><category><![CDATA[musings]]></category><category><![CDATA[blaggregator]]></category><pubDate>Thu, 28 Dec 2017 00:08:20 GMT</pubDate></item><item><title><![CDATA[Winter break priorities, 2017-18]]></title><description><![CDATA[<p>So I just wanted to write something up real fast to document my development priorities over my upcoming winter break (which starts tomorrow, whoohoo!). This is just generally useful to have, because I'm forgetful, but will also help keep me accountable since the list is public.</p>
<p>So, here are the projects I want to get done, in rough order of priority:</p>
<ol>
<li>Polish up an <a href="https://github.com/nodejs/node/pull/14164#issuecomment-325553163">outstanding Node.js Pull Request</a> that automatically generates manpages for every module shipped with Node core</li>
<li>Deal with GPG key signing which I have been procrasinating on for, well, years now</li>
<li>Push some Debian packaging work forward (I've done some work to get the <a href="https://tracker.debian.org/pkg/profanity">Profanity packaging</a> up-to-date, and I want to get <a href="https://github.com/strugee/filter-other-days">filter-other-days</a> into Debian as well)</li>
<li>Finish up v1 of <a href="https://github.com/strugee/lazymention">lazymention</a> - high-priority since it's so close to being done!</li>
<li>Finally push through and implement <a href="https://github.com/pump-io/pump.io/issues/1241">ActivityPub in pump.io core</a></li>
<li>Clear <a href="https://github.com/pump-io/pump.io/pulls">the backlog of PRs</a> sent to pump.io</li>
</ol>
<p>I also am setting a goal of writing at least two blog posts per week, and I want to try to get some editing done on a paper I'm planning to try to get published (though I'm not sure if I'll end up doing this, honestly).</p>
<p>That seems like a lot, honestly, and yet I feel like I must have missed something - I will update this list if I think of anything else!</p>
<p>Cheers, and happy holidays everyone! \o/</p>
]]></description><link>https://strugee.net/blog/2017/12/winter-break-priorities-2017-18</link><guid isPermaLink="true">https://strugee.net/blog/2017/12/winter-break-priorities-2017-18</guid><category><![CDATA[personal]]></category><category><![CDATA[blaggregator]]></category><pubDate>Wed, 20 Dec 2017 17:44:18 GMT</pubDate></item></channel></rss>