<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[strugee.net blog - 'security' category]]></title><description><![CDATA[strugee.net blog - 'security' category]]></description><link>https://strugee.net/blog/category/security</link><generator>stratic-indexes-to-rss</generator><lastBuildDate>Wed, 30 Jan 2019 04:55:03 GMT</lastBuildDate><atom:link href="https://strugee.net/blog/category/security/index.rss" rel="self" type="application/rss+xml"/><copyright><![CDATA[© Copyright 2012-2018 AJ Jordan. Available under the GNU Affero GPL.]]></copyright><webMaster><![CDATA[AJ Jordan <alex@strugee.net>]]></webMaster><item><title><![CDATA[New temporary signing keys]]></title><description><![CDATA[<p>So unfortunately, recently I have lost my Nitrokey, which I liked very much. In addition to this being fairly upsetting, I am now left with the sticky situation of not being able to sign code - while I have a master key (from which I can generate new subkeys), I'm currently at college and my copy of the key is sitting 3,000 miles away at home.</p>
<p>To get around this situation, I've generated a temporary signing keypair. This keypair is set to expire after 3 months (and I don't intend to renew it). When I have access to my master keypair, I will revoke the old subkeys, generate new subkeys (it was probably time, anyway) and revoke the temporary keypair.</p>
<p>The new fingerprint is <code>D825FD54D9B940FF0FFFB31AA4FDB7BE12F63EC3</code>. I have uploaded the key to <a href="https://github.com/strugee.gpg">GitHub</a> as well as the Ubuntu keyserver and <code>keys.gnupg.net</code> (just as my original was). The key is also incorporated into my Keybase account so that you can bootstrap trust in it, if you want to verify software signatures or whatever.</p>
]]></description><link>https://strugee.net/blog/2018/04/new-temporary-signing-keys</link><guid isPermaLink="true">https://strugee.net/blog/2018/04/new-temporary-signing-keys</guid><category><![CDATA[personal]]></category><category><![CDATA[security]]></category><category><![CDATA[blaggregator]]></category><pubDate>Tue, 17 Apr 2018 23:01:33 GMT</pubDate></item><item><title><![CDATA[Improving GPG security]]></title><description><![CDATA[<p>Recently I've been putting some effort into improving the security of my <a href="https://strugee.net/gpg">GPG key</a> setup, and I thought I would take a moment to document it since I'm really excited!</p>
<h1>Nitrokey</h1>
<p>First, and most importantly, I have recently acquired a <a href="https://www.nitrokey.com/">Nitrokey Storage</a>. After I initialized the internal storage keys (which took a <em>really</em> long time), I used <code>gpg --edit-key</code> to edit my local keyring. I selected my first subkey, since in my day-to-day keyring the master key's private component is stripped, and issued <code>keytocard</code> to move the subkey to the Nitrokey. Then I repeated the process for the other subkey.</p>
<p>In the middle of this I <em>did</em> run into an annoying issue: GPG was giving me errors about not having a pinentry, even though the <code>pinentry-curses</code> and <code>pinentry-gnome3</code> packages were <em>clearly</em> installed. I had been dealing with this issue pretty much since I set up the system, and I had been working around it by issuing <code>echo "test" | gpg2 --pinentry-mode loopback --clearsign &gt; /dev/null</code> every time I wanted to perform a key operation. This worked because I was forcing GPG to not use the system pinentry program and instead just prompt directly on the local TTY; since I had put in the password, <code>gpg-agent</code> would then have the password cached for a while so I could do the key operation without GPG needing to prompt for a password (and thus without the pinentry error). However, this didn't seem to work for <code>--edit-key</code>, which I found supremely annoying.</p>
<p>However this turned out to be a good thing because it forced me to <em>finally</em> deal with the issue. I tried lots of things in an effort to figure out what was going on: I ran <code>dpkg-reconfigure pinentry-gnome3</code>, <code>dpkg-reconfigure gnupg2</code>, and I even manually ran <code>/usr/bin/pinentry</code> to make sure it was working. Turns out that, like many helpful protocols, the pinentry protocol lets you send <code>HELP</code>, and if you do so you'll get back a really nice list of possible commands. I played around with this and was able to get GNOME Shell to prompt me for a password, which was then echoed back to me in the terminal!</p>
<p>Despite feeling cool because of that, I still had the pinentry problem. So finally I just started searching all the GPG manpages for mentions of "pinentry". I looked at <code>gpg(1)</code> first, which was unhelpful, and then I looked at <code>gpgconf(1)</code>. That one was also <em>mostly</em> unhelpful, but the "SEE ALSO" section <em>did</em> make me think to look at <code>gpg-agent(1)</code>, where I hit upon the solution. Turns out <code>gpg-agent(1)</code> has a note about pinentry programs right at the very top, in the "DESCRIPTION" section:</p>
<blockquote>
<p>Please make sure that a proper pinentry program has been installed under the default filename (which is system dependent) or use the option <code>pinentry-program</code> to specify the full name of that program.</p>
</blockquote>
<p>The mention of the <code>pinentry-program</code> option led me pretty immediately to my solution. I had originally copied my <code>.gnupg</code> directory from my old MacBook Pro, and apparently GPGTools - a Mac package that integrated GPG nicely with the environment (as well as providing a GUI I never used) - had added its own <code>pinentry-program</code> line to <code>gpg-agent.conf</code>. That line pointed at a path installed by GPGTools, which of course didn't exist on my new Linux system. As soon as I removed the line, <code>--edit-key</code> worked like a charm. (I've also just added <code>gpg-agent.conf</code> to my <a href="https://github.com/strugee/dots/blob/master/.gnupg/gpg-agent.conf">dotfiles</a> so I notice this kind of thing in the future.)</p>
<p>So far, I'm really enjoying my Nitrokey. It works really well and the app is pretty good, although the menu can be pretty glitchy sometimes. I've used the password manager for a couple high-security passwords (mostly bank passwords) which is great, and I've switched my two-factor authentication for GitHub from FreeOTP on my phone to the Nitrokey since GitHub is a super important account and I really want to make sure people can't push code as me.</p>
<p>There are only two problems I've had with the Nitrokey so far. The first is that it's slow. I notice a significant pause when I do any crypto operation, probably somewhere between a half a second to a second. This hits me quite often since I sign all my Git commits; however I suspect I'll get used to this, and the security benefits are well worth the wait anyway. The other problem is that the Nitrokey doesn't support FIDO U2F authentication. This wasn't a surprise (I knew it wouldn't when I was shopping models) but is nevertheless a problem I would like to deal with (which means getting a second device). The basic reason is just that U2F is newer than the Nitrokey I have. Other than those, though, I would highly recommend Nitrokey. The device is durable, too - I just carry it around in my pocket. (I briefly considered putting it on my keychain - for those of you who haven't met me in person, I have my keychain on an easily-detachable connector attached to a belt loop - but I decided against it because my keychain is kinda hefty.)</p>
<h1>Keybase</h1>
<p>In addition to the Nitrokey, I've also finally started using <a href="https://keybase.io/">Keybase</a>!</p>
<p>For a long time I wasn't too sure about Keybase. I felt like people should really be meeting in person and doing keysigning parties, and I didn't like that they encourage you to upload a private key to them, even if it's password-protected. Eventually I softened my position a little bit and got an invite from <a href="https://keybase.io/yawnbox">Christopher Sheats</a> (back then you needed an invite) but I only made it halfway through the install process before getting distracted and forgetting about it for, you know, several years.</p>
<p>This time, though, I decided to finally get my act together. Do I still kinda think it's a bummer that Keybase encourages private key uploads? Sure. Are real-life keysignings better? <em>Absolutely</em>. But even though they're better, a lot of experience trying to do them and teach them has thoroughly convinced me that they're just too impractical. There are lots of people who might need to at least have <em>some</em> trust in my key - for example, to verify software signatures - and this is a pretty decent solution for them. Not to mention a novel and interesting solution. Plus, it's possible to use Keybase in such a way that you're not compromising security in any way, which is the way I do it.</p>
<p>So tl;dr: I'm on the Keybase bandwagon now. <a href="https://keybase.io/strugee">My profile</a> is also now linked to from my <a href="https://strugee.net/gpg">GPG keys</a> page.</p>
<h1>Safe for master key</h1>
<p>Finally, my dad's wife's safe has recently been moved into our house and is conveniently sitting next to my computer. Currently, I keep my master key in a file on a flash drive with an encrypted LUKS container. When I need to access my master key, this file gets unlocked with <code>cryptsetup</code> and then mounted somewhere on my laptop, and I pass the <code>--homedir</code> option to <code>gpg</code> to point it at the mount location. This is better than just keeping the master lying around day-to-day, but still pretty unideal as I'm exposing it to a potentially compromised, non-airgapped computer. Therefore I plan to get a Raspberry Pi (or something similar) and put it in the safe so I can use it as a fully trusted computer that's never been connected to the internet (and is therefore <em>very</em> hard to compromise). I'll keep the Pi in the safe to provide greater assurance that it hasn't been tampered with, as well as to provide a physical level of redundancy for the key material's security. This will hopefully happen Real Soon Now™ - I can't wait!</p>
]]></description><link>https://strugee.net/blog/2018/01/improving-gpg-security</link><guid isPermaLink="true">https://strugee.net/blog/2018/01/improving-gpg-security</guid><category><![CDATA[security]]></category><category><![CDATA[personal]]></category><category><![CDATA[blaggregator]]></category><pubDate>Tue, 16 Jan 2018 23:01:13 GMT</pubDate></item><item><title><![CDATA[filter-other-days: Artificial Ignorance-compatible logfile date filtering]]></title><description><![CDATA[<p>I've just published version 1.0 of my latest project, <code>filter-other-days</code> - a shell script to filter logfiles for today's date in an Artificial Ignorance-compatible way.</p>
<p>If you haven't heard of <a href="http://www.ranum.com/security/computer_security/papers/ai/index.html">Artificial Ignorance</a>, it's something you should look into cause it's pretty awesome. Here's the tl;dr: it doesn't make sense to look for all the "interesting" things  in logfiles, because it's not actually possible to enumerate all the failure conditions of a system. So instead what we do is <em>throw away</em> entries that we're <em>sure</em> are just routine. Since we've gotten rid of all the uninteresting entries, whatever is left has to be interesting.</p>
<p>I find this pretty compelling, and decided to start implementing it on my Tor relay. I quickly realized that my ideal workflow would be to configure cron to send me email with a daily report of interesting log entries. However, this presented a problem: how to get just today's log entries? I wanted to be able to handle all logfiles at once instead of receiving different reports for different logs, so I had to be able to parse all logfiles in the same way. My relay runs on FreeBSD, so the logs are unstructured text files, and even worse, several daemons (like Tor itself) write timestamps in a different format - this makes parsing all logfiles at once super difficult because I couldn't just trivially <code>grep</code> for today's date since that would end up dropping legitimate entries from logfiles that formatted their timestamps differently.</p>
<p>I briefly considered trying to write a regex to match all sorts of different timestamp formats, but quickly rejected this idea as too fragile. There are a lot of moving parts in a modern operating system - what if e.g. a daemon changed its defaults about how to format timestamps? Or, more likely, what if I simply missed a particular format present in my logs? Then I'd be accidentally throwing away an entire logfile. To solve this problem, I decided to apply the same idea behind Artificial Ignorance - if I couldn't reliably, 100% match log entries from today's date, I could do the next best thing and attempt to discard all entries from <em>other</em> dates. In this case the worst that could happen is me recieving irrelevant information, and I'd be basically guaranteed to never miss an legitimate entry from today.</p>
<p><code>filter-other-days</code> is a working implementation of this design. Originally I put it with the other random scripts I keep with my <a href="https://github.com/strugee/dots/tree/master/bin">dotfiles</a>, but it quickly became obvious that it was useful as a standalone project. So I <a href="https://github.com/strugee/dots/commit/7dd7e2755c55194cdff1c7b24b24bca72581e346">extracted</a> it into its own repository, which now lives <a href="https://github.com/strugee/filter-other-days">on GitHub</a>. From there I continued to improve the script while adding a test suite and writing extensive documentation (including a Unix manpage - I always feel like a wizardly hacker when writing those things). This took, by my estimation, somewhere between 10 and 15 hours because this is actually a shockingly non-trivial problem, but mostly because regexes are hard.</p>
<p>But today I finally finished! So I'm super excited to announce that version 1.0 of <code>filter-other-days</code> is now available. You can either clone it from GitHub or download a <a href="https://github.com/strugee/filter-other-days/releases/tag/v1.0.0">tarball</a> (and the accompanying signature, if you want). It works pretty well already, but I have some ideas for future directions the project could go:</p>
<ol>
<li>Logic allowing you to actually specify the date you want to filter for, instead of assuming it's today (though you actually can already get this behavior using <code>faketime</code>; that's what the test suite does)</li>
<li>Removal of the dependency on GNU <code>seq</code> - this is, to my knowledge, the only non-POSIX requirement of <code>filter-other-days</code></li>
<li>Debian package, maybe?</li>
<li>More log formats (please <a href="https://github.com/strugee/filter-other-days/issues">report bugs</a> if you have formats <code>filter-other-days</code> doesn't recognize - which you probably do!)</li>
</ol>
<p>If you find this project useful, let me know! I'd love to hear about how people are using it. Or if it breaks (or doesn't fill your usecases), please <a href="https://github.com/strugee/filter-other-days/issues">report bugs</a> or send patches - I love those, too! Either way, may the logs be with you!</p>
]]></description><link>https://strugee.net/blog/2017/10/announcing-filter-other-days</link><guid isPermaLink="true">https://strugee.net/blog/2017/10/announcing-filter-other-days</guid><category><![CDATA[development]]></category><category><![CDATA[security]]></category><category><![CDATA[sysadmin]]></category><category><![CDATA[releases]]></category><category><![CDATA[blaggregator]]></category><pubDate>Fri, 20 Oct 2017 19:19:51 GMT</pubDate></item><item><title><![CDATA[pump.io denial-of-service security fixes now available]]></title><description><![CDATA[<p>Recently some denial-of-service vulnerabilities were discovered in various modules that pump.io indirectly depends on. I've bumped Express and <code>send</code> to pull in patched versions, and I've updated our fork of <code>connect-auth</code> to require a patched version of Connect, too. The remaining vulnerabilities I've confirmed don't affect us.</p>
<p>Because of these version bumps, I've just put out security releases which all administrators are encouraged to upgrade to as soon as possible. A semver-major release (5.0.0) was released within the past 6 months so per our <a href="https://github.com/pump-io/pump.io/wiki/Security">security support policy</a> this means there are three new releases:</p>
<ol>
<li>pump.io 5.0.2 replaces 5.0.0 and is available now on npm</li>
<li>pump.io 4.1.3 replaces 4.1.2 and is available now on npm</li>
<li>pump.io 4.0.2 will replace 4.0.1 <strike>and is currently undergoing automated testing (it'll be on npm shortly)</strike> <strong>Update:</strong> pump.io 4.0.2 is now on npm</li>
</ol>
<p>As these are security releases we encourage admins to upgrade as soon as possible. If you're on 5.0.0 installed via npm - our recommended configuration - you can upgrade by issuing:</p>
<pre><code>$ npm install -g pump.io@5
</code></pre>
<p>If you're on 4.1.3, you can upgrade by issuing:</p>
<pre><code>$ npm install -g pump.io@4
</code></pre>
<p>And when 4.0.2 is out, if you're on 4.0.1 you can upgrade by issuing:</p>
<pre><code>$ npm install -g pump.io@4.0
</code></pre>
<p>Note though that 4.1.3 is a drop-in replacement for 4.0.2, so you should consider just upgrading to that instead. Or even better, <a href="https://pumpio.readthedocs.io/en/latest/upgrades/4.x-to-5.x.html">upgrade to 5.x</a>!</p>
<p>If you don't have an npm-based install, you'll have to upgrade however you normally do. How to do this will depend on your particular setup.</p>
<p>As always, if you need help, you should get in touch with <a href="https://github.com/pump-io/pump.io/wiki/Community">the community</a>. I'd also like to specifically thank <a href="https://identi.ca/jxself">Jason Self</a>, who generously deployed a 24-hour private beta of these fixes on <a href="https://datamost.com/">Datamost</a>. One of the version bumps was ever-so-slightly risky, and being able to test things in production before rolling out patches for the entire network was invaluable. I wouldn't be as confident as I am in these releases without his help. So thanks, Jason - I really appreciate it!</p>
]]></description><link>https://strugee.net/blog/2017/10/denial-of-service-security-fixes-now-available</link><guid isPermaLink="true">https://strugee.net/blog/2017/10/denial-of-service-security-fixes-now-available</guid><category><![CDATA[pump.io]]></category><category><![CDATA[development]]></category><category><![CDATA[security]]></category><category><![CDATA[releases]]></category><category><![CDATA[blaggregator]]></category><pubDate>Sun, 01 Oct 2017 17:40:59 GMT</pubDate></item><item><title><![CDATA[Default-secure systems]]></title><description><![CDATA[<p>So recently I presented on <a href="https://strugee.net/presentation-operational-security/">operational security</a> and then started in on the nightmare that is <a href="https://strugee.net/presentation-https-deployment/">HTTPS deployment</a>. And like I did with <a href="https://strugee.net/blog/2017/01/new-programming-language-part-i-handlers">language-level security</a> (I <em>still</em> need to write part 2 of that post), I thought to myself, this is so difficult. Why isn't there something that will do this for me? That's what my latest project is.</p>
<p>Here's the tl;dr:</p>
<pre><code>type(app)
=&gt; Django/Express.js/etc. app
secure_system(app)
=&gt; Docker image
</code></pre>
<p>Or in other words, you'll be able to take an existing web app that you've written, run it through this system, and it will spit out a complete, reasonably-secure system image.</p>
<p>Let's step back.</p>
<h1>The status quo</h1>
<p>Currently, when a developer wants to run a web app, they can either use something like Heroku, which is fully manged, or a VM from DigitalOcean or Amazon EC2 or something. There are a variety of reasons you might not want to use Heroku, but the only other option is a VM - and with a VM, you get a bare system where you have to set up everything from scratch. Lots of developers just don't have the operational experience to do this properly or securely, but it's not like they can go and get an operations team to do it for them. So they end up with systems that may have active security problems as well as little to no defense-in-depth mitigations for when security inevitably fails. Security is just another operational concern the developer has no time and no expertise to deal with, so it just doesn't happen. The developer spins up a VM, gets it to where it "works" and then moves on. <strong>This is not good enough.</strong></p>
<p>I don't want to create a false dichotomy, though: this is not the developer's fault. Everyone has conflicting priorities and it's unreasonable to expect the developer to spend lots of time learning to administrate systems so that they can then spend even more time, you know, administering systems. The problem is that there just isn't enough options available - we have to provide something better.</p>
<h1>A middle ground</h1>
<p>This is what my project is about: creating a middle ground between fully-managed deployment platforms and barebones, setup-from-scratch VMs.</p>
<p>This project rests on the idea that operational security (at least, in a single-server, single-admin context) flows from consistency, least privilege, and proactive, defense-in-depth security measures. Here are a couple core design goals:</p>
<ol>
<li>Meet developers where they are. Configuration management like Puppet is a great way to enforce consistency, but it adds a level of indirection and is just another thing that people running hobbyist projects don't have time to learn.</li>
<li>Tight integration with apps - this excludes more obscure types of web applications, but gives us a better footing to set up a solid deployment environment. It also may let us integrate more tightly with things like Content Security Policy in the future.</li>
<li>Support virtual hosting. The ability to run multiple apps while paying for a single VM is a compelling reason people go with VMs over e.g. Heroku - we won't be helping anyone if we leave this out.</li>
<li>Upgrades are optional. Any system image created by this project will present a system that is organized and can be maintained and modified by hand without breaking everything.</li>
<li>Upgrades are possible. Tight app framework integration will aid with putting data into well-known places that can be backed up and migrated to a new image generated by a newer version of this system.</li>
<li>Not designed for "real" production environments. Any project that has a dedicated operations person should not be using this; they should be rolling their own custom environments with something like Puppet. Accordingly, there won't be compromises in security in favor of flexibility - it's designed to cover 75% of cases "pretty well", which is still better than the status quo for smaller projects (almost 100% of cases don't have any security at all).</li>
</ol>
<p>I'd also like to highlight one really important decision: the output is complete system images. Probably at first this will mean Docker containers but this could easily be turned into VM images. This is a critical part of the design because it allows us to make broad, sweeping changes - for example, preferring system components written in memory-safe languages, replacing OpenSSL with LibreSSL, or creating systemd unit files that lock down service runtime environments to reduce the impact of a compromise. These improvements aren't possible unless we control the whole system. And because upgrades are optional but possible, the developer can get security improvements by "just" upgrading a component that they use, in the same way that they'd upgrade a library or something, as opposed to security being a continuous process they have to worry about. Again, obviously not perfect - but much better than the status quo.</p>
<p>I hope to have a MVP out Real Soon Now™. But in the meantime, if you have thoughts, feel free to reach out.</p>
]]></description><link>https://strugee.net/blog/2017/03/default-secure-systems</link><guid isPermaLink="true">https://strugee.net/blog/2017/03/default-secure-systems</guid><category><![CDATA[development]]></category><category><![CDATA[security]]></category><category><![CDATA[blaggregator]]></category><pubDate>Mon, 13 Mar 2017 15:59:17 GMT</pubDate></item><item><title><![CDATA[New programming language part I: Handlers]]></title><description><![CDATA[<p>So my latest project at the <a href="https://recurse.com">Recurse Center</a> is a new programming language, as yet unnamed. Basically this was inspired by my <a href="https://strugee.net/presentation-security-design">security design</a> presentation in which I laid out a couple ways you can improve programs' security designs, like process separation. And it occurred to me: why is this so difficult? Something should be doing this for me. Enter... whatever the hell I end up calling my language.</p>
<p>One of the most important building blocks of the language is something that I'm currently calling a "Handler". A Handler is basically a segregated piece of application code that handles some task or problem domain. For example, a Handler for outgoing DNS requests (syntax subject to change, obviously):</p>
<pre><code>Handler DNS {
    function getAddressFromHostname(hostname) {
        // Very much pseudocode - all function calls here are example OS calls

        sendDnsQuery(hostname);
        while (!haveDnsResult()) {
            sleep(1);
        }
        
        return getDnsResult();
    }
}
</code></pre>
<p>This Handler has one simple method, <code>getAddressFromHostname()</code>. It sends a DNS query, blocks until it has a result, and then returns the result.</p>
<p>What's cool about this Handler is that it will be run in its own process - in other words, each Handler is automatically transformed into a privilege-separated process. However, <code>getAddressFromHostname()</code> can still be called from other, high-level Handlers! The language will do all the data marshalling for you, so from a language perspective it looks like a regular function call even though in practice it's going cross-process.</p>
<p>Now, let's improve our Handler a little bit. It's pretty unfortunate that we can only make one DNS request at a time (since it's blocking), so let's use an event loop.</p>
<pre><code>Handler DNS is eventLoop {
    function getAddressFromHostname(hostname) {
        // Still pseudocode

        return sendDnsQuery(hostname, function(address) {
            return address;
        });
    }
}
</code></pre>
<p>We specified that the <code>DNS</code> Handler is an event loop, so the language automatically set up a Node-style event loop - we never actually called anything to enter an event loop, it just sort of happened as a construct of the language. Note also the nice async-aware return syntax - the result of <code>sendDnsQuery()</code> is returned to <code>getAddressFromHostname()</code>'s caller, and the result of <code>sendDnsQuery()</code> is specified by the return value of the anonymous function callback.</p>
<p>We can improve correctness even more by specifying that the Handler is not allowed to make syncronous I/O calls at all:</p>
<pre><code>Handler DNS is eventLoop, async {
    // ...
}
</code></pre>
<p>Handlers let you pick and choose different elements and design choices for different parts of your application. For example, if you had a Handler responsible for processing data, it might make sense to restrict it to being entirely functional:</p>
<pre><code>Handler incomingData is functional {
    // ...
}
</code></pre>
<p>In such a Handler, any functions with side effects wouldn't be callable, enforced at compile-time (possibly parse-time, depending on whether I can make it compiled or not given the type system). Perhaps you want to spawn a new sandbox process for each piece of incoming data (OpenSSH does this, for example, when first receiving authentication data from untrusted users):</p>
<pre><code>// The `ephemeral` keyword will probably be something better, but for now...
Handler incomingData is functional, ephemeral {
    // ...
}
</code></pre>
<p>If your application requires root - for example, if you were writing an NTP daemon that needed to call <code>setTimeOfDay()</code> - that's also specified at the Handler level:</p>
<pre><code>Handler incomingData is root {
    // ...
}
</code></pre>
<p>This should give you some idea of why Handlers are really interesting, even beyond the process separation concept that underlies them. (It's also worth noting that while I've focused mostly on daemons, this can also be used to securely implement e.g. <code>file</code>.) I'm really excited to get these ideas out there so I'll stop for now, but pretty soon I'll write another blog post talking about the type system.</p>
]]></description><link>https://strugee.net/blog/2017/01/new-programming-language-part-i-handlers</link><guid isPermaLink="true">https://strugee.net/blog/2017/01/new-programming-language-part-i-handlers</guid><category><![CDATA[personal]]></category><category><![CDATA[security]]></category><category><![CDATA[blaggregator]]></category><pubDate>Thu, 26 Jan 2017 10:59:20 GMT</pubDate></item><item><title><![CDATA[Surveillance priorities]]></title><description><![CDATA[<p>For several years now I've been really interested in technology policy - things like security, privacy and censorship, and especially how those things relate to both mass surveillance and freedom-respecting software. That's why I follow organizations like Fight for the Future and the EFF, and why I e.g. participated in the movement to stop SOPA and PIPA, the internet censorship bills.</p>
<p>But a week or so ago I had a realization: I'm not interested in surveillance law anymore.</p>
<p>It's clear to me that Congress is completely busted. The 113th Congress came <a href="http://www.pewresearch.org/fact-tank/2014/12/29/in-late-spurt-of-activity-congress-avoids-least-productive-title/">very, very close</a> to being the least productive Congress in modern history. Our current Congress isn't particularly good either, although they are (as far as I know) not as bad as the 113th - but they're still not good enough that I'm confident in their ability to actually, you know, pass laws. Even if we <em>could</em> get Congress to pass laws at all, it's unclear whether we could actually get them to pass laws curtailing mass surveillance. Over and over again we see Congress trying to pass misguided laws that weaken encryption, damage the DNS, and do all sorts of other seriously nasty (and hacky!) things - it just doesn't seem very reasonable to me to assume that they'd change their minds and decide to do (what we think is) the right thing[1].</p>
<p>This is why I'm not interested in surveillance law anymore. I find it to be a waste of time. Instead, I've shifted my focus towards systems that are fundamentally designed to resist surveillance and censorship. That's why I advocate for <a href="https://whispersystems.org/">Signal</a> and why I work on <a href="http://pump.io">pump.io</a>: because these are both systems designed from the ground up to, among other things, essentially be unaffected by surveillance law. Who cares if Congress passes a law that says they can surveil pump.io users? Congress saying a bunch of words doesn't change the fact that technically speaking, that's quite hard to do. Certainly it's more difficult than surveilling e.g. Facebook.</p>
<p>As Moxie Marlinspike puts it in <a href="https://youtu.be/xIiklPyS8MU?t=33m54s">this talk</a> on PKI's flaws and an alternative system called Convergence:</p>
<blockquote>
<p>And, you know, with this legislation that's been coming up recently like COICA and PROTECT IP and this kind of thing, you know - to me the real lesson here isn't whether this passes or not because there's been, you know, some kind of heroic efforts to prevent this legislation from going through. But I think, you know, the thing to take away from this is that they're <em>trying</em>. To pass regulation that messes with this stuff. And maybe one day they'll succeed.</p>
</blockquote>
<p>Trying to make Congress do the right thing is, I feel, akin to an endless arms race: they don't seem to be getting the message and it's doubtful that they'll stop in the near- or medium-term future.</p>
<p>A much better solution is this: implement secure-by-default, freedom-respecting, encrypted and/or federated systems, and be done. Forever.</p>
<p> [1]: honestly, I think a big problem with this is that a lot of Congress is old white guys. Emphasis on old. The problem of people in the legal sphere not understanding technology, especially technology relating to security, privacy and encryption, has cropped up before. Consider, for example, the judge who <a href="https://nakedsecurity.sophos.com/2016/07/01/judge-decides-we-dont-have-any-right-to-privacy/">ruled</a> that a Tor user had "no reasonable expectation of privacy" because he literally could not wrap his head around how Tor worked and what the FBI did.</p>
]]></description><link>https://strugee.net/blog/2017/01/surveillance-priorities</link><guid isPermaLink="true">https://strugee.net/blog/2017/01/surveillance-priorities</guid><category><![CDATA[politics]]></category><category><![CDATA[privacy]]></category><category><![CDATA[security]]></category><category><![CDATA[musings]]></category><pubDate>Mon, 23 Jan 2017 16:55:27 GMT</pubDate></item><item><title><![CDATA[Pump.io 1.0.0 is now available!]]></title><description><![CDATA[<p><a href="https://github.com/e14n/pump.io/releases/tag/v1.0.0">Pump.io 1.0.0</a> is officially available! Whoooo!</p>
<p>I just wanted to write up an announcement real quick to celebrate. Here's a sample what's gone into this release:</p>
<ul>
<li>Node 4.x support</li>
<li>Lots of security improvements including a better cross-site scripting scrubber and security-related headers that help protect the web UI (most notably, the web UI now declares a Content Security Policy)</li>
<li>Minor improvements to the API to make it (slightly) smarter</li>
<li><a href="https://www.gnu.org/software/librejs/">LibreJS</a> support</li>
<li>Numerous dependency upgrades, most notably Connect</li>
<li>And of course, tons of minor bugfixes and improvements</li>
</ul>
<p>For more details, see the brand-new <a href="https://github.com/e14n/pump.io/blob/master/CHANGELOG.md#100---2016-08-26">change log</a>.</p>
<p>And of course since we're now past 0.x.x releases, we're now officially making a commitment to the community to make only API-compatible changes going forward (or at least, until 2.0.0!).</p>
<p>As this release <em>does</em> improve security and fixes a lot of bugs, node administrators are encouraged to upgrade as soon as possible. If you have a global, npm-based install, you can upgrade with:</p>
<pre><code>sudo npm install -g pump.io
</code></pre>
<p>And with a source-based install:</p>
<pre><code>git pull
git checkout v1.0.0
npm install --production
</code></pre>
<p>If you're upgrading from 0.3.0, everything should Just Work(tm). Don't forget to restart your daemon!</p>
<p>One final note - the rumors are true. While we're not doing so <em>yet</em>, we are, in fact, planning to deprecate running under Node.js 0.10 and 0.12 very soon. Also, if you upgrade to Node.js 4.x early, the new, better XSS scrubber will be enabled - <em>however</em>, be aware that pump.io is far less tested under Node.js 4.x and you are likely to run into more bugs than you would under 0.10 or 0.12. This is an unfortunate situation, but sadly there's really nothing to be done about it. :(</p>
<p>Special thanks to Menno Vossen, Laura Arjona, Evan Prodromou, Jan Kusanagi and all the other volunteers who did so many different things to make this release happen. It truly wouldn't have happened without you.</p>
<p>Enjoy the release!</p>
<p>With &lt;3,</p>
<p>AJ</p>
]]></description><link>https://strugee.net/blog/2016/08/pump.io-1.0.0-is-now-available</link><guid isPermaLink="true">https://strugee.net/blog/2016/08/pump.io-1.0.0-is-now-available</guid><category><![CDATA[development]]></category><category><![CDATA[releases]]></category><category><![CDATA[security]]></category><category><![CDATA[pump.io]]></category><pubDate>Fri, 26 Aug 2016 23:37:54 GMT</pubDate></item></channel></rss>