<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[strugee.net blog - 'tor' category]]></title><description><![CDATA[strugee.net blog - 'tor' category]]></description><link>https://strugee.net/blog/category/tor</link><generator>stratic-indexes-to-rss</generator><lastBuildDate>Tue, 04 Dec 2018 08:46:00 GMT</lastBuildDate><atom:link href="https://strugee.net/blog/category/tor/index.rss" rel="self" type="application/rss+xml"/><copyright><![CDATA[Â© Copyright 2012-2018 AJ Jordan. Available under the GNU Affero GPL.]]></copyright><webMaster><![CDATA[AJ Jordan <alex@strugee.net>]]></webMaster><item><title><![CDATA[RC week 10]]></title><description><![CDATA[<p>This is week 10 of being at the <a href="https://recurse.com">Recurse Center</a>.</p>
<script async="" defer="" src="https://www.recurse-scout.com/loader.js?t=3d49e64361d4b897ffd2fd56dcd93ca4"></script>
<h2>Day 35</h2>
<p>Arrived ~13:00, departed ~21:30, total time at RC 8h30m.</p>
<p>Decided (in the morning) to sleep in because honestly, I was just really behind on sleep.</p>
<p>Didn't do a whole lot today. Took care of the monthly pump.io release, but spent most of the day writing <a href="https://strugee.net/blog/2017/03/driftless-at-1000-mph">Driftless at 1,000 mph</a>. In the evening (i.e. after the Iron Blogger meeting) I spent some time fixing ejabberd's configuration to use the new access control syntax (I'd rewritten the config a while back, but hadn't deployed it yet because it broke logins).</p>
<h2>Day 36</h2>
<p>Arrived ~10:40, departed ~23:40, total time at RC 13h0m.</p>
<p>Spent basically all day working on <a href="https://github.com/strugee/realtime.recurse.com">realtime.recurse.com</a>. I (mostly) finished up the bits that watch the filesystem and dispatch events (including the "periodic" submission logic), then started in on an automatic update mechanism. I'm pretty pleased with how it's turning out - I think it's pretty elegant. And, it's secure - updates are required to be cryptographically signed by yours truly. Went out to Black Burger with a <em>bunch</em> of people before going to Fat Cat in the evening. Then came back, worked on the updater a little more, and went home.</p>
<h2>Day 37</h2>
<p>Arrived ~14:40, departed ~23:20, total time at RC 8h40m.</p>
<p>Overslept by accident this morning. Spent a bit of time in the afternoon dealing with email, then focused on realtime.recurse.com - basically I was just working on the autoupater I started yesterday. My Python is <em>definitely</em> improving!</p>
<p>I'm actually really pleased with the updater. It's pretty elegant, I think. Basically whenever the server sees a request coming from the client, it checks the <code>User-Agent</code> header to see if the client's out of date and, if so, sends back an <code>X-Requires-Upgrade</code> header. Upon receiving this header the client will go fetch version information, which it'll use to download and verify an update bundle cryptographically signed by me. Yay for secure updates, and yay for simplicity! (Note that this design basically just reuses the connections the client is already making to the server, so it doesn't have to poll for updates all the time.)</p>
<p>I also spent a couple hours talking with Mikhail, discussing a lot of things - ranging from how Node.js's event loop works to the <code>is</code> keyword in Python to static site generator architecture compared to dynamic site architecture.</p>
<h2>Day 38</h2>
<p>Arrived ~9:50, departed ~23:00, total time at RC 13h10m.</p>
<p>Woke up, completely naturally, around 7 AM despite going to sleep at 3 AM. This was so surprising - and this is a true story - that I thought I had woken up at 7 PM and missed the entire day, including Security Club, Abstract Salad Factory, and Thursday night presentations. I was <em>really</em> mad, honestly. But then I looked at my watch and realized that I was on 24-hour time but it didn't say "19:00" and also my alarms were in the future and my phone was in 24-hour time too and also Anja on Zulip said "?" when I said I'd slept through Security Club. Despite the overwhelming evidence in the end, though, I still had a weird feeling that it was 7 PM. So that's the story of how, for a good 5 or 10 minutes, I genuinely believed I'd slept through the entire day.</p>
<p>Once I got to RC, I spent the morning finally(!) merging in a bunch of upstream <code>ejabberd.yml</code> config changes to steevie's ejabberd configuration, which got me closer to fixing the awful spam problem I have. Then I went to Abstract Salad Factory, followed by Security Club. Then in the afternoon (and after presentations) I started reading <a href="https://www.freebsd.org/">FreeBSD</a> documentation since that's what I'm running my new Tor relay on - as I discovered a couple days ago, my old one apparently got hung during boot and was consuming 100% CPU due to the kernel image being corrupted or something. I chose FreeBSD because a) it's a rock-solid system, b) it's a good opportunity to gain experience with administrating a BSD, and c) it increases the diversity of the Tor network. Also, had a conference call in the afternoon with the <a href="https://www.eff.org/">EFF</a> and Paul from <a href="https://ta3mseattle.org/index.php/Main_Page">TA3M Seattle</a> about TA3M Seattle joining the EFF's <a href="https://www.eff.org/electronic-frontier-alliance">Electronic Frontier Alliance</a>, which was exciting for everyone.</p>
<h2>Friday</h2>
<p>Arrived ~13:00, departed ~22:30, total time at RC 9h30m. As always, Friday doesn't count as a day because RC is technically not in session.</p>
<p>Didn't do a whole lot of coding. Spent a while helping <a href="https://jxself.org/">Jason</a> debug <a href="https://datamost.com/">Datamost</a>'s 3.0.0 upgrade (which apparently broke uploads). Attended presentations for the RC Game Jam, then fixed the documentation that caused Jason's problems. Spent a little bit of time polishing the website and README, too.</p>
<p>In the evening, fixed people being banned from ejabberd MUCs, then proceeded to fix my spam problem. Whoooooo! Then I kept working on my Tor relay.</p>
<h2>Executive summary</h2>
<p>Like any week, this had moments where I wasn't very productive. But overall I think it was pretty good - I made a lot of progress on realtime.recurse.com (and improved my Python in the process), and I made a lot of progress on setting up my Tor relay again (and learned a bunch about FreeBSD in the process). Also, I fixed my ejabberd spam problem. I learned nothing from that, but thank <em>god</em> I did it because the spam problem was honestly <em>awful</em>. The one issue was that I just didn't do a very good job getting up and making it to checkins.</p>
<p>Total time at RC 52 hours 50 minutes; cumulative time 464 hours 50 minutes.</p>
]]></description><link>https://strugee.net/blog/2017/03/rc-week-10</link><guid isPermaLink="true">https://strugee.net/blog/2017/03/rc-week-10</guid><category><![CDATA[personal]]></category><category><![CDATA[tor]]></category><category><![CDATA[blaggregator]]></category><pubDate>Wed, 15 Mar 2017 18:42:40 GMT</pubDate></item><item><title><![CDATA[Revisiting my Tor relay]]></title><description><![CDATA[<p>(Okay, so I <em>miserably</em> failed my blog-every-day thing. Shut up. Maybe next time I'll try every week or something... anyway.)</p>
<p>A couple of days ago I logged into the <a href="https://atlas.torproject.org/#details/710E9E3A0A443E3FD33D2801298042783CAD2EAE">Tor relay I run</a> to show someone the ARM graphs. I had a fair amount of traffic, so the graphs were fairly impressive, but I'm also in the habit of running <code>apt-get update; apt-get upgrade</code> every time I log into a server, so I did that too. To my surprise, I got a message telling me that there was a dependency problem with my kernel! So like the great sysadmin I am, I looked at such a fundamental system problem, shrugged my shoulders, and said, "oh, I should probably fix that". And then logged out.</p>
<p>Well, I did end up fixing it today. And boy, was it an adventure. My first step was to ignore the APT problems and edit my <code>torrc</code>, to reflect a) the fact that I'm not eligible for the AWS Free Tier anymore (so I needed to throttle bandwidth), b) my new email, and c) my new GPG key. With that being done, I knew that I could easily have the system fix dependency problems by doing a simple <code>apt-get install -f</code>. Easy!</p>
<p>Well, no. That tried to install some Linux kernel headers, which seemed all well and good, until I got this:</p>
<pre><code>Unpacking linux-headers-3.2.0-90 (from .../linux-headers-3.2.0-90_3.2.0-90.128_all.deb) ...
dpkg: error processing /var/cache/apt/archives/linux-headers-3.2.0-90_3.2.0-90.128_all.deb (--unpack):
unable to create `/usr/src/linux-headers-3.2.0-90/arch/arm/plat-pxa/include/plat/dma.h.dpkg-new' (while processing `./usr/src/linux-headers-3.2.0-90/arch/arm/plat-pxa/include/plat/dma.h'): No space left on device
No apport report written because the error message indicates a disk full error
dpkg-deb: error: subprocess paste was killed by signal (Broken pipe)
</code></pre>
<p>Um, what? How am I out of free space? Okay, whatever. I knew that there were probably a lot of packages cached in <code>/var/cache/apt/</code>, including old, vulnerable packages that had been replaced by the unattended upgrades system. I did an <code>ls</code>, and found only about five <code>.deb</code> files - something must have been automatically cleaning that directory. I was getting a little worried now, but I nuked the files anyway and reran <code>apt-get install -f</code>. <em>Same thing</em>. Well, okay, maybe I didn't get rid of enough stuff? How much did I need?</p>
<pre><code>$ df -h /
Filesystem      Size  Used Avail Use% Mounted on
/dev/xvda1      4.0G  2.2G  1.6G  59% /
</code></pre>
<p>At this point I'm in full-on "something-is-seriously-wrong-and-I-<em>need</em>-to-recover" mode. How was it possible that I had only used 59% of the filesystem, but <code>dpkg</code> was saying my disk was full? A little searching the internet later, I found the culprit:</p>
<pre><code>$ df -i
Filesystem     Inodes  IUsed IFree IUse% Mounted on
/dev/xvda1     262144 257479  4665   99% /
udev            74758    377 74381    1% /dev
tmpfs           76179    259 75920    1% /run
none            76179      3 76176    1% /run/lock
none            76179      1 76178    1% /run/shm
</code></pre>
<p>I hadn't run out of disk space. But I <em>had</em> run out of inodes. (Isn't this supposed to happen to <em>other</em> people?)</p>
<p>I tried removing some stuff via APT, but that refused to do anything due to the dependency problems. My next thought was that there were probably a bunch of old processes running that were essentially holding a bunch of inodes hostage. I couldn't install <code>debian-goodies</code>, so I couldn't use <code>checkrestart</code>, but I improvised by looping over all running services in a for loop, and restarting them.</p>
<p>Still nothing.</p>
<p>I'm not proud of what I did next. But I was backed into a corner, so I did something only <code>dpkg</code> is supposed to do. I ran <code>rm -r</code> on a couple directories in <code>/usr/src</code>. And boy, it was like magic. Suddenly <code>apt-get install -f</code> worked like a charm. It started to upgrade a couple packages, rebuilding some GRUB configuration files... and then came to a screeching halt.</p>
<pre><code>Setting up linux-headers-3.2.0-90-virtual (3.2.0-90.128) ...
dpkg: dependency problems prevent configuration of linux-headers-virtual:
linux-headers-virtual depends on linux-headers-3.2.0-68-virtual; however:
Package linux-headers-3.2.0-68-virtual is not installed.
dpkg: error processing linux-headers-virtual (--configure):
dependency problems - leaving unconfigured
No apport report written because the error message indicates its a followup error from a previous failure.
dpkg: dependency problems prevent configuration of linux-virtual:
linux-virtual depends on linux-headers-virtual (= 3.2.0.68.81); however:
Package linux-headers-virtual is not configured yet.
dpkg: error processing linux-virtual (--configure):
dependency problems - leaving unconfigured
No apport report written because the error message indicates its a followup error from a previous failure.
Errors were encountered while processing:
linux-headers-virtual
linux-virtual
E: Sub-process /usr/bin/dpkg returned an error code (1)
</code></pre>
<p>Are you kidding?? <em>More</em> errors?</p>
<p>Turns out that APT is essentially the only thing on this system that makes large changes to the filesystem. So the probability that APT would be the program to trigger the inode limit was pretty high. It started an upgrade run, then got interrupted in the middle by the "no space left on device" error, leaving the dependency tree in a state that we in the tech community call "100% totally screwed". (This is the technical term.)</p>
<p>I'll spare you the gory details, but I ended up trying to chase down packages in the Ubuntu archive, running <code>ubuntu-support-status</code> beacuse I was wondering if the packages I was looking for actually <em>weren't in</em> the archive, because they were unsupported, using <code>aptitude</code> instead of <code>apt-get</code> (because <code>aptitude</code>'s dependency resolver tends to be better), etc. Finally the solution turned out to be doing <code>dpkg --install</code> on the exact right <code>.deb</code>s in the exact right order, which finally satisfied APT's dependency woes, allowed <code>apt-get install -f</code> to fix the configuration problems, and allowed the hundreds of packages which had been waiting for an upgrade to <em>finally</em> install. Whew!</p>
<p>Anyway, I need to upgrade the version of Ubuntu the system is on (currently it's 12.04.5 LTS), because Tor is out of date (among other reasons). However, since that will involve taking the system down for a reboot, I wanted to memorialize the following:</p>
<pre><code>$ uptime
00:01:47 up 392 days, 17:15,  1 user,  load average: 0.05, 0.04, 0.05
</code></pre>
<p>Holy moly. This system is bordering on 400 days of uptime. That's over a year of continuous run time! Astonishing.</p>
<p>Wish me luck with this upgrade...</p>
<p><strong>tl;dr</strong>: inode limits are <em>killer</em>.</p>
]]></description><link>https://strugee.net/blog/2015/09/revisiting-my-tor-relay</link><guid isPermaLink="true">https://strugee.net/blog/2015/09/revisiting-my-tor-relay</guid><category><![CDATA[sysadmin]]></category><category><![CDATA[tor]]></category><pubDate>Fri, 04 Sep 2015 16:34:29 GMT</pubDate></item></channel></rss>